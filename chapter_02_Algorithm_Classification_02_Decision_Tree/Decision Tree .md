## 의사결정 나무 알고리즘

의사결정 나무(Decision Tree) 알고리즘은 주어진 학습데이터를 통해 트리구조를 만들고 생성한 트리구조를 통해 데이터를 분류하거나 수치를 예측하는 알고리즘을 말한다. <span style="font-size:70%">1</span> 

대부분의 머신러닝 알고리즘이 결과를 도출하기 까지의 과정 및 해석이 불가능한데 반해,  의사결정트리(Decision Tree)는 결과를 도출하는 과정을 직관적으로 확인 및 해석할 수 있고 굉장히 범용적으로 사용가능하고 또 높은성능을 보이기 때문에 굉장히 많이 사용하는 머신러닝 알고리즘이다. <span style="font-size:70%">2</span> 

### 01. 트리구조에 대한 이해

트리구조는 계층적(hierarchy)인 구조를 갖는 정보를 표현하는 하나의 자료구조이다. 크게 그래프구조라는 자료구조의 한 종류이기 때문에 노드(Node)와 엣지(Edge)로 자료를 계층적으로 표현한다. 

쉽게 생각해 노드는 데이터를 담는 그릇이고, 각 노드 끼리의 어떻게 연결되었는지에 대한 관계를 엣지로 표현한다고 생각하면 편하다. 이렇게 자료를 표현한 모습이  나무를 거꾸로 메달아 놓은것과 비슷하다고 하여 트리구조라 부른다.

의사결정 나무는 트리구조 중 이진트리(binary tree)이다. 이진트리는 하나의 부모노드(parents node)가 반드시 2개 이하의 자식노드(children node)를 갖는 트리를 말한다. 이 때 모든 데이터가 존재하는 가장 첫번째 노드를 뿌리노드(root node)라고 부르며 뿌리노드 부터 자식노드로 계속 데이터가 분할된다. 이 때 더이상 자식노드가 없는 가장 마지막 노드를 잎노드(leaf node)라 부른다. <span style="font-size:70%">3</span> 

> 구체적인 그래프 구조 및 트리구조에 대하여 확인이 필요하면, 그래프구조 > 트리구조 로 구글링하여 찾아볼 수 있다.



데이티에 있는 규칙을 학습을 통해 자동으로 찾아내 트리(Tree) 기반의 분류 규직을 만드는 것입니다. 일반적으로 규칙을 가장 쉽게 표현하는 방법은 IF/ELSE 기반으로 나타내는 것인데, 쉽게 생각하면 스무고개 개임과 유사하며 룰 기반의 프로그램에 적용되는 IF, ELSE 를 자동으로 찾아내 예측을 위한 규칙을 만드는 알고리즘으로 이해하면 더 쉽게 다가올 것입니다. 따라서 데이티의 어떤 기준을 바탕으로 규칙을 만들어 야가장 효율적인 분류가 될 것인가가 알고리즘의 성능을 크게 좌우합니다.

다음 그림은 결정 트리의 구조를 간략하게 나타낸 것입니다. 규칙 노드(Decision Node)로 표시된 노드는 규칙 조건이 되는 것이고, 리프 노드(Leaf Node)로 표시된 노드는 결정된 클래스 값입니다. 그리고 새로운 규칙 조건마다 서브 트리(Sub Tree)가 생성됩니다. 데이터 세트에 피처가 있고 이러한 피처가 결합해 규칙 조건을 만들 때마다 규칙 노드가 만들어집니다. 하지만 많은 규칙이 있다는 것은 곧 분류를 결정하는 방식이 더욱 복잡해진다는 얘기이고, 이는 곧 과적합으로 이어지기 쉽습니다. 즉, 트리의 깊이(depth)가 깊어질수록 결정 트리의 예측 성능이 저하될 가능성이 높습니다.

가능한 적은 결정 노드로 높은 예측 정확도를 가지려면 데이터를 분류할 때 최대한 많은 데이터 세트가 해당 분류에 속할 수 있도록 결정 노드의 규칙이 정해져야 합니다. 이를 위해서는 어떻게 트리를 분할(Split)할 것인가가 중요한데 최대한 균일한 데이터 세트를 구성할 수 있도록 분할하는 것이 필요합니다.

먼저 균일한 데이터 세트가 어떤 것을 의미하는지 조금 자세히 설명하겠습니다. 다음 그림에 가장 균일한 데이터 세트부터 순서대로 나열한다면 어떻게 될까요?

답은 C가 가장 균일도가 높고 그다음 B, 마지막으로 A 순입니다. C의 경우 모두 검은 공으로 구성되므로 데이터가 모두 균일합니다. B의 경우는 일부 하얀 공을 가지고 있지만, 대부분 검은 공으로 구성되어 다음으로 균일도가 높습니다. A의 경우는 검은 공 못지않게 많은 하얀 공을 가지고 있어 균일도가 제일 낮습니다. 이러한 데이터 세트의 균일도는 데이터를 구분하는 데 필요한 정보의 양에 영향을 미칩니다. 가령 눈을 가린 채 데이터 세트 C에서 하나의 데이터를 뽑았을 때 데이터에 대한 별다른 정보 없이도'검은 공'이라고 쉽게 예측할 수 있습니다. 하지만 A의 경우는 상대적으로 혼잡도가 높고 균일도가 낮기 때문에 같은 조건에서 데이터를 판단하는데 있어 더 많은 정보가 필요합니다.

결정 노드는 정보 균일도가 높은 데이터 세트를 먼저 선택할 수 있도록 규칙 조건을 만듭니다. 즉, 정보 균일도가 데이터 세트로 쪼개질 수 있도록 조건을 찾아 서브 데이터 세트를 만들고, 다시 이 서브 데이터 세트에서 균일도가 높은 자식 데이터 세트 쪼개는 방식을 자식 트리로 내려가면서 반복하는 방식으 로 데이터 값을 예측하게 됩니다. 예를 들어 박스 안에 30개의 레고 블록이 있는데, 각 레고 블록은 '형태' 속성으로 동그라미, 네모, 세모, '색깔' 속성으로 노랑, 빨강, 파랑이 있습니다. 이 중 노랑색 블록의 경우 모두 동그라미로 구성되고, 빨강과 파랑 블록의 경우는 동그라미, 네모, 세모가 골고루 섞여 있다 고 한다면 각 레고 블록을 형태와 색깔 속성으로 분류하고자 할 때 가장 첫 번째로 만들어져야 하는 규칙 조건은 if 색깔 =='노란색'이 될 것입니다. 왜냐하면 노란색 블록이면 모두 노란 동그라미 블록으로 가장 쉽게 예측할 수 있고, 그다음 나머지 블록에 대해 다시 균일도 조건을 찾아 분류하는 것이 가장 효율적인 분류 방식이기 때문입니다.
이러한 정보의 균일도를 측정하는 대표적인 방법은 앤트로피를 이용한 정보. 이득(Information Gain)
지수와 지니 계수가 있습니다.

정보 이득은 엔트로피라는 개념을 기반으로 합니다. 엔트로피는 주어진 데이터 집합의 혼잡도를 의미하는데, 서로 다른 값 이 섞여 있으면 엔트로피가 높고, 같은 값이 섞여 있으면 엔트로피가 낮습니다. 정보 이득 지수는 1에서 엔트로피 지수를 뺀 값입니다. 즉. 1- 엔트로피 지수입니다. 결정 트리는 이 정보 이득 지수로 분할 기준을 정합니다. 즉, 정보 이득이 높은 속성을 기준으로 분할합니다.

지니 계수는 원래 경제학에서 불평등 지수를 나타낼 때 사용하는 계수입니다. 경제학자인 코라도 지니(Corrado Gin)의 이름에서 딴 계수로서 0이 가장 평등하고 1로 갈수록 불평등합니다. 머신러닝에 적용될 때는 지니 계수가 낮을수록 데이터 균일도가 높은 것으로 해석해 지니 계수가 낮은 속성을 기준으로 분할합니다.

결정 트리 알고리즘을 사이킷런에서 구현한 Decision TreeClassifier는 기본으로 지니 계수를 이용해 데이터 세트를 분할합니다. 결정 트리의 일반적인 알고리즘은 데이터 세트를 분할하는 데 가장 좋은 조건, 즉 정보 이득이 높거나 지니 계수가 낮은 조건을 찾아서 자식 트리 노드에 걸쳐 반복적으로 분할한 뒤, 데이터가 모두 특정 분류에 속하게 되면 분할을 멈추고 분류를 결정합니다.

결정 트리 모델의 특징
결정 트리의 가장 큰 장점은 정보의 균일도라는 룰을 기반으로 하고 있어서 알고리즘이 쉽고 직관적이 라는 점입니다. 결정 트리가 룰이 매우 명확하고, 이에 기반해 어떻게 규칙 노드와 리프 노드가 만들어 지는지 알 수 있고, 시각화로 표현까지 할 수 있습니다. 또한 정보의 균일도만 신경 쓰면 되므로 특별한 경우를 제외하고는 피처의 스케일링과 정규화 같은 전처리 작업이 필요 없습니다. 반면에 결정 트리 모델의 가장 큰 단점은 과적합으로 정확도가 떨어진다는 점입니다. 피처 정보의 균일도에 따른 룰 규칙으로 서브 트리를 계속 만들다 보면 피처가 많고 균일도가 다양하게 존재할수록 트리의 깊이가 자요 복잡해질 수밖에 없습니다.

모든 데이터 상황을 만족하는 완벽한 규칙은 만들지 못하는 경우가 오히려 더 많음에도 불구하고 결정트리는 학습 데이터 기반 모델의 정확도를 높이기 위해 계속해서 조건을 추가하면서 트리 깊이가 계속 커지고, 결과적으로 복잡한 학습 모델에 이르게 됩니다. 복잡한 학습 모델은 결국에는 실제 상황에 (테스트 데이터 세트) 유연하게 대처할 수 없어서 예측 성능이 떨어질 수밖에 없습니다. 차라리 모든 데이 더 상황을 만족하는 완벽한 규칙은 만들 수 없다고 먼저 인정하는 편이 더 나은 성능을 보장할 수 있습 니다. 즉, 트리의 크기를 사전에 제한하는 것이 오히려 성능 튜닝에 더 도움이 됩니다.

결정 트리 장점
• 쉽다. 직관적이다
• 피처의 스케일링이나 정규화 등의 사전 가공 영향도가 크지 않음.
결정 트리 단점
• 과적합으로 알고리즘 성능이 떨어진다. 이를 극복하기 위해 트리의 크기를 사전에 제한하는 튜닝 필요.

3.6 의사결정나무 학습
의사결정 나무 분류기는 우리가 모델의 해석을 신경 써야 할 때 매력적인 모델이다.
의사결정 나무라는 이름이 시사하듯이, 이 모델은 연속된 질문을 기준으로 의사결정을 함으로써 데이터를 쪼개어 가는 것으로 생각할 수 있다.
 특정한 날의 활동을 기반으로 의사결정나무를 사용한 다음의 사래를 살펴보자.
우리 훈련 데이터의 피처들에 근거하여 의사결정나무 모델은 샘플의 분류 레이블을 추론하기 위해 연속된 질문을 학습한다. 비록 다음 그림은 범주형 변수를 근거로 하 는 의사결정나무의 개념을 설명했지만 이와 같은 개념을 우리의 피처에도 적용할 수 있다. 이것은 피처들이 아이리스(iris) 데이터에서 처럼 실수인 경우에도 잘 작동한다.
예를 들면, sepal width 피처 축을 따라 컷오프를 정의해서 "sepal width )= 2.8cm 와 같은 바이너리 질문을 할 수 있을 것이다.

의사결정 알고리즘을 사용하면 나무의 뿌리부터 시작해서 가장 큰 IGinforation galin) 값을 도출하는 피처에 대한 데이터를 분리한다. 이것은 다음 절에서 좀 더 자세 하게 설명할 것이다. 각각의 자식 노드에서 잎들(eaves)이 순수해질 때까지 이러한 분리 과정을 반복할 수 있다. 이것은 각 노드의 샘플들은 모두 같은 분류에 속한다는 것을 의미한다. 실제로 이것은 많은 노드를 갖는 아주 깊은 나무(deep tree) 결과가 발 생할 수 있고, 그 결과 쉽게 오버피팅될 수 있다. 그렇기 때문에, 보통 나무의 최대 깊 이에 대한 제한을 설정함으로써 가지치기 하고자 한다. 

<hr>
미주
<span style="font-size:70%">1</span>  :  R을 활용한 기계학습, 브레드란츠, 전철욱, 에이콘, 164p ~169p를 참고, 데이터마이닝 기법 비교 연구: 단일 및 복수 의사결정나무, 신은주, 장남식, 363p를 참고

<span style="font-size:70%">2</span> :  R을 활용한 기계학습, 브레드란츠, 전철욱, 에이콘, 164p ~165p를 참고, 파이썬 머신러닝 완벽 가이드, 권민철, 위키북스, 183p 참고

<span style="font-size:70%">3</span> : Kmook 강의자료를 참고하여 작성함.
http://contents.kocw.or.kr/KOCW/document/2016/cup/weonsumghyun/8.pdf
http://contents.kocw.or.kr/document/lec/2010/33/01/08.pdf

참고문헌
* 도서 
	* 파이썬 머신러닝 완벽 가이드, 권민철, 위키북스, 183p ~209p
	* 파이썬 머신러닝, 세바스타안 라슈카, 이혜연, 지앤선 79p ~ 87p 

* 논문 및 학술지
	* 데이터마이닝 기법 비교 연구: 단일 및 복수 의사결정나무, 신은주, 장남식 