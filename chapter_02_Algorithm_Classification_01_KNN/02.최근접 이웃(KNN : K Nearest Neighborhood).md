## 02.최근접 이웃(KNN : K Nearest Neighborhood)

### (1) KNN 이해하기
옛말에 "유유상종"<span style="font-size:70%">[1]</span>이기 때문에 "그 사람을 알려면 그가 사귀는 친구를 보라"<span style="font-size:70%">[2]</span> 라는 말이 있다. 서로 비슷한 사람들이 끼리끼리 어울려 다니며 사귀는법이기 때문에 어울리는 무리를 통해 그 사람에 대해서 판단해볼만 하다는 이야기이다. 이는 KNN알고리즘의 작동 방식과 정확하게 일치한다. 

KNN은 어떤 미지의 데이터에 대하여 가장 근접한 데이터의 라벨을 그 데이터의 정체로 판단하는 지도학습 분류(Classification) 알고리즘을 말한다. 

<div align="center">
	<img src="https://github.com/int29/PMLP-101/blob/main/chapter_02_Algorithm_Classification_01_KNN/02_01.png?raw=true" width=500>
	<p>[그림201]</p>
</div>

### (2) KNN의 작동원리

KNN은 어떤 미지의 라벨을 분류하는 지도학습 방식의 알고리즘이기 때문에, 분류하고자 하는 정답값이 존재하는 라벨링이 된 데이터를 모델학습에 활용한다.

KNN은 학습시 별도의 복잡한 과정 없이 훈련데이터 모두를 N차원상의 벡터공간의 한 점(벡터)으로 메모리에 저장하고 학습을 마무리한다. 아래 그림처럼 총 10개의 관측값이 존재하는 라벨링된 데이터를 학습할 경우 10개의 점을 N차원상의 한 점으로 맵핑할 뿐이다.

<div align="center">
	<img src="https://github.com/int29/PMLP-101/blob/main/chapter_02_Algorithm_Classification_01_KNN/02_02.png?raw=true" width=700>
	<p>[그림202]</p>
</div>

그리고 분류할 데이터도 마찬가지로 훈련데이터와 같은 벡터공간의 벡터로 맵핑하고, 사전에 맵핑시킨 모든 벡터와 거리를 비교한다.

그 후 가장 가까운 K개만큼의 벡터를 추출해 그 중 다수결 원칙에 따라 분류를 진행한다.

<div align="center">
	<img src="https://github.com/int29/PMLP-101/blob/main/chapter_02_Algorithm_Classification_01_KNN/02_03.png?raw=true" width=400>
	<p>[그림203]</p>
</div>

그렇기 때문에 K의 수에 따라 모델이 분류하는 결과값이 달라질 수 있으며, 보통은 K값의 동률을 방지하기 위해 홀수로 설정하며, 만약 동률이 발생할 경우 무작위 혹은 각 점의 거리가 짧은 순으로 판단한다.

<div align="center">
	<img src="https://github.com/int29/PMLP-101/blob/main/chapter_02_Algorithm_Classification_01_KNN/02_04.png?raw=true" width=600>
	<p>[그림204]</p>
</div>

### (3)KNN 알고리즘 더 깊게 이해하기

### KNN 알고리즘의  장단점
KNN은 단순히 데이터를 N차원상의 벡터공간의 한 점으로 저장하고, 비교하여 예측값을 제공할 뿐이기 때문에 [01.머신러닝의 이해](https://github.com/int29/PMLP-101/blob/main/chapter_01_Understanding_Machine_Learning/01.%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0.md) 에서 머신러닝의 학습에 대해 설명했던 추상화와 일반화 과정이 없는 모형이다. 이렇게 모델을 만들지 않고 단순히 각 데이터를(인스턴스) 저장하였다 활용하는 알고리즘을 인스턴스 베이스 머신러닝 알고리즘이라고 하고, KNN은 인스턴스 베이스 모델중 한가지 이다.<span style="font-size:70%">1</span>,<span style="font-size:70%">2</span> 

KNN은 학습과정에서 추상화를 통해 데이터의 패턴을 계산하고 미지의 데이터에 대해서도 잘 반영할 수 있는 모델을 만드는 일반화 과정 없이 단순히 벡터공간상의 한 점으로 맵핑하여 학습을 진행하기 때문에 모델 베이스 머신러닝에 비해서 훈련속도가 빠르다. 또한 간단한 알고리즘이기 때문에 하이퍼 파라미터가 적어 개발 및 파라미터 튜닝과정 또한 상대적으로 빨리 모델을 개발할 수 있다. 마지막으로 로직이 직관적이기 때문에 머신러닝 모델이 예측한 결과에 대해 약한수준의 해석력을 제공한다. 때문에 가장 처음 생성하고 살펴보는 베이스모형(혹은 벤치마크모형)에 많이 사용하곤한다.

반면 분류하려는 값에 대하여 모든 훈련 인스턴스와 1:N비교를 진행하기 때문에 훈련속도는 빠르지만 예측(분류)하는 속도는 모델 베이스 모형보다 느리고 두 벡터 사이의 거리를 계산하는 방식을 갖고 있기 때문에 이상치나 잡음, 데이터 피처(Feature)의 스케일에 영향을 많이 받는다는 단점이 존재한다. 또한 적절 K를 선정하는것이 어렵다. 

### KNN의 거리계산 방식
KNN은 거리가 가까울수록 유사도가 높고, 거리가 멀수록 유사도가 낮다는 굉장히 심플한 논리를 갖고 있다. 따라서 KNN은 벡터(좌표평면의 한 점)끼리의 거리를 계산하여 얼마나 유사한지 유사도를 계산한다. 거리 계산 시 가장 많이 사용하는 방법은 유클리디안 거리(Euclidean distance) 계산 방식으로 중학교 수학에 나오는 피타고라스의 정리와 계산방법이 동일한 거리계산 법이다. 

> 유클리디안 거리(Euclidean distance)는 유클리드가 선형대수에서는 L2 norm이라고 부르는 벡터간 최단거리를 계산하는 방법이다. 세분화된 분야에 따라서 부르는 명칭은 다르지만 계산하는 방식은 모두 동일하다.

유클리디안 거리(Euclidean distance) 는 아래와 같이 계산한다.

$$d(p,q) = \sqrt{\sum_{i=1}^n (q_i - p_i)^2} =  \sqrt{(x_1 - x_2)^2+(y_1 - y_2)^2+...+(u_1 - u_2)^2} $$
$d(p,q)$는 두 점 $p$와 $q$ 사이의 거리를 나타내고, $n$은 차원의 수를 나타냄. $u$는 N번째 차원의 축을 의미함

예를 들어 x축과 y축 2개의 축을 갖는 2차원상의 두 점(벡터) $p$와 $q$는 각각 $(x_1, y_1)$, $(x_1, y_2) = (1, 2), (3, 4)$ 이라는 좌표를 갖는다고 해보자.  두 점 $p$와 $q$는 아래와 같이 좌표계에 표현이 가능할 것이다.

<div align="center">
	<img src="https://github.com/int29/PMLP-101/blob/main/chapter_02_Algorithm_Classification_01_KNN/02_05.png?raw=true" width=400>
	<p>[그림205]</p>
</div>

이 때 두 점사이의 최단거리는 두점의 직선거리가 되기 때문에 

<div align="center">
	<img s6rc="https://github.com/int29/PMLP-101/blob/main/chapter_02_Algorithm_Classification_01_KNN/02_06.png?raw=true" width=400>
	<p>[그림206]</p>
</div>

유클리디안 거리 계산 공식에 대입하면 두 점사이의 거리가 $2$ 임을 계산할 수 있다.
$$d(p,q) =\sqrt{(1 - 3)^2+(2 - 4)^2}= \sqrt{(-2)^2+(-2)^2} = \sqrt{4} = 2$$

이를 쉽게 생각하면 아래와 같은 삼각형의 대각선의 거리와 동일한 모양이 나온다. 따라서 피타고라스의 정리를 적용하면 동일한 결과가 나오는것을 알 수 있다.

<div align="center">
	<img s6rc="https://github.com/int29/PMLP-101/blob/main/chapter_02_Algorithm_Classification_01_KNN/02_07.png?raw=true" width=400>
	<p>[그림207]</p>
</div>

> 거리를 계산하는 방법이나, 유사도를 계산하는 방법은 맨하튼 거리(L1 norm), 코사인유사도($\theta$)(cosin distance), 두 점사이의 상관관계를 활용하는 마할라노비스 거리(Mahalanobis distance) 등 여러 방법이 존재한다. 각 방법의 장단이 존재하고 사용해야하는 상황이 각기 다르기 때문에 추후에 꼭 각 거리의 계산법과 장단점들을 공부하길 바란다.<span style="font-size:70%">[3]</span> 

### KNN데이터 활용해보기

데이터 전처리 데이터에 활용하는 데이터의 설명은 [예제 데이터 설명 : iris 데이터]() 를 통해 꼭 확인후에 알고리즘 활용을 살펴보길 바란다.



<hr>
### 인용
<span style="font-size:70%">[1]</span> : 네이버 국어사전(https://ko.dict.naver.com/#/entry/koko/dcc68b744c1b410dae38939990808f7c) 

<span style="font-size:70%">[2]</span> :  不知其人이어든 視其友하라 (http://www.hanjanews.com/news/articleView.html?idxno=3650)
> 不知其子이어든 視其父하고 不知其人이어든 視其友하라.
> [독음]부지기자 시기부 부지기인 시기우[해석] 그 아들을 모르겠거든 그 아비를 보고 그 사람을 모르겠거든 그 친구를 보라

<span style="font-size:70%">[3]</span> : 데이터 과학을 위한 통계 : 데이터 분석에서 머신러닝까지 50가지 핵심개념(2018), 피터 브루스, 앤드루 브루스, 이준용, 김태헌, 한및미디어, 242p 
><원문>
>벡터사이의 거리를 측정하기 위해 수많은 거리 지표들이 있다. 수치 데이터를 다룰 때, 마할라비스 거리 Mahalanobis distance는 두 변수 간의 상관관계를 사용하기 때문에 .... <후략>

### 미주
<span style="font-size:70%">1</span> : 반대로 추상화와 일반화 과정을 거쳐 특정 모형을 생성하여 학습후 해당 모형을 통해 예측하는 모형을 모델 베이스 머신러닝(model based machine learning)이라 부른다. 이러한 모델 베이스 머신러닝은 특정 구조의 모형을 가정하고, 그 성질을 파라미터로 표현하는 존재하는 parametric 모형과 특정 구조의 모형을 가정하지 않는  모형인 non-parametric모형 중 어떤 모형을 활용하는지에 따라  parametric machine learning 과 non-parametric machine learning 으로 분류한다. 

<span style="font-size:70%">2</span> : 또는 게으른 학습(Lazy learning) 알고리즘이라고도 부른다.
