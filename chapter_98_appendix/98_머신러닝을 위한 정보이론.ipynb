{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edec78ff",
   "metadata": {},
   "source": [
    "## 98. 머신러닝을 위한 정보이론(Information Theory)<span style=\"font-size:70%\"> 1</span>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59d0abd",
   "metadata": {},
   "source": [
    "우리가 신호를 송신해주고 돈을 받는 사업을하고 있다고 생각해보자 우리의 고객들은 비용을 지불하여 다양한 정보를 송신하고 싶어한다. 1900년대다 보니 지금처럼 스마트폰이 있는 세계는 아니고 모스기계를 통해 메시지를 상대방에게 전달한다.\n",
    "\n",
    "모스신호기는 0과 1로만 신호를 표현하여 전달한다.\n",
    "<br><br>\n",
    "<div align=\"center\">\n",
    "<img src=\"https://cphoto.asiae.co.kr/listimglink/1/2016092716332909423_1.jpg\" width=\"300px\">\n",
    "\t<p>[그림98_01]</p>\n",
    "</div>\n",
    "<br><br>\n",
    "우리에게 고객 3명이 찾아왔다. 고객 A는 동전던지기 결과를 전달하고 싶어하고, 고객 B는 동전2개를 던지는 내기를 위해 결과를 전달하고 싶어한다. 고객 A, B 에게 각각 얼마를 받아야할까? 싯가라고 별 기준없이 즉흥적으로 비용을 받으면 장사의 신용은 떨어지고 경쟁업체가 득세할 것이다.\n",
    "\n",
    "우리는 고기를 구매하던 생선을 구매하던지 kg이라는 기준을 통해 물품의 가격을 측정하고 이 기준을 통해 산정된 결과를 지불한다. 이처럼 정보 또한 하나의 물리량으로 측정하여 전달하는것이 합리적일 것이다. \n",
    "\n",
    "정보 이론은 1948년 벨 연구소의 클로드 섀넌(Claude Shannon)이 “A mathematical theory of communication”이라는 논문을 발표하여 시작된 이론으로 엔트로피라는 개념을 통해 메시지의 정보량을 정량적으로 측정하는 방법에 대해 다루는 응용수학의 한 분야이다.<span style=\"font-size:70%\">[1]</span><span style=\"font-size:70%\">[2]</span> 즉 킬로그램(Kg)이나, 리터(L)처럼 메시지속 정보의 양을 정보 엔트로피라는 수치로 계산하는 방법에 대해 연구하는 것이다. \n",
    "\n",
    "> 정보의 가치를 그저 감성적으로 이해하던 것을 1945년경 미국의 섀넌(Shannon)은 정보의 가치를 엔트로피(entropy)라는 것으로 계량화하였다. 어떤 정보의 엔트로피란 주어진 정보를 될 수 있는 데로 압축하여 0과 1로 표현할 때 필요한 평균 비트 수이다. 엔트로피를 통하여 정보를 질량이나 에너지, 운동량과 같은 수량화된 물리량으로 만들었다. <span style=\"font-size:70%\">[2]</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca692b00",
   "metadata": {},
   "source": [
    "### (1)  메시지의 이진수 표현 \n",
    "\n",
    "고객 A,B가 전송하고자 하는 정보의 양을 일단 0과 1로 전달할 수 있도록 변경을 해야한다. 불가능하다면 전송 자체가 안되기 때문이다.\n",
    "\n",
    "고객 A부터 시작해보자 \n",
    "\n",
    "    고객A : 동전1개 던지기 내기 결과를 전송하여 돈을 청구하고 싶습니다.\n",
    "\n",
    "고객 A는 동전던지기의 결과를 전달하기를 원한다. 동전던지기는 나올 수 있는 경우의수는 앞면과 뒷면 두가지 뿐이고 이를 0과 1로 표현하면 아래와 같이 표현할 수 있을것이다.\n",
    "\n",
    "    [앞]: 0\n",
    "    [뒤]: 1\n",
    "\n",
    "고객 B는 동전2개를 던자는 결과를 전달하기를 원한다. 동전2개의 결과는 경우의 수가 더 많기 때문에 조금 더 복잡해졌다. 한번 0과1을 내맘대로 조합해서 표현해보자\n",
    "\n",
    "    고객B : 동전2개 던지기 내기 결과를 전달해 내기결과를 전달하고 싶습니다.\n",
    "\n",
    "    [앞,앞]: 00\n",
    "    [앞,뒤]: 01\n",
    "    [뒤,앞]: 10\n",
    "    [뒤,뒤]: 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edfbc6d",
   "metadata": {},
   "source": [
    "### (2) 신호 전송횟수 : 비트(bit)\n",
    "이제 한번 기준을 생각해보자, 0과1로만 전달해야하는데 고객A는 어떤 결과가 나오던지간에 한번의 전송으로 표현이 가능하다. 앞면이 나왔다면 0, 뒷면이 나왔다면 1을 전송하면 되는것이다. 반면에 고객 B는 적어도 2번의 전송이 필요하다.\n",
    "\n",
    "우리는 이렇게 신호를 전달하는 횟수를 비트라 부르기로 했고 Kg처럼 비트를 통해 요금을 산정하기로 정한다면 사업자인 우리와 비용을 지불하는 고객들은 별 불만이 없을 것이다.\n",
    "\n",
    "    <고객 A> \n",
    "    * 결과를 1 회 전송 = 1bit\n",
    "    [앞]: 0\n",
    "    [뒤]: 1\n",
    "    \n",
    "    <고객 B> \n",
    "    * 결과를 1회 전송 = 2bit\n",
    "    [앞,앞]: 00\n",
    "    [앞,뒤]: 01\n",
    "    [뒤,앞]: 10\n",
    "    [뒤,뒤]: 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12f9337",
   "metadata": {},
   "source": [
    "모든 경우의 수를 2진수로 표현하기 때문에 동일한 정보를 아래 이진트리로 표현하면 더 직관적인데, 이를 통해 우리가 정보를 전달하기 위해 필요한 전송횟수, 즉 비트수는 $2^{n}$=전송하는 경우의 수로 나타낼수 있는 알 수 있다. 즉 $n$을 구하면 몇번을 통해 전송해야 하는지 bit로 계산할 수 있는 것이다.<br><br>\n",
    "\n",
    "<br><br>\n",
    "<div align=\"center\">\n",
    "<img src=\"https://github.com/int29/PMLP-101/blob/main/chapter_98_appendix/img/98_01.png?raw=true\" width=\"350px\">\n",
    "\t<p>[그림98_02]</p>\n",
    "</div>\n",
    "<br><br>\n",
    "\n",
    "만약에 26개로 이루어진 알파벳 한글자를 전송하기 위해 필요한 전송횟수(비트수)는 $2^{n}=26$로 표현할 수 있고 $n$을 구하면 알파벳 한글자를 전송하는데 필요한 비트수를 계산할 수 있게된다. 우리는 $n$값을 계산하는 쉬운 방법을 우리는 알고 있다. $log_{2}{4}=2$ 라는 의미는 2를 몇 제곱해야지 4가 되는지에 대한 표기이다. 즉 $2^{n}=26$ 는 $log_{2}{26}=n$ 으로 표현할 수 있고 $log_{2}{26}=4.7$ 정도 이기 때문에 경우에 따라서 4번 혹은 5번 전송이 필요하게 된다. 즉 5글자의 경우 $4.7bit\\cdot5=23.5 bit$가 될것이다. \n",
    "\n",
    "이렇게 단일 메시지를 이진수로 표현하는데 필요한 최소 자리수, 즉 비트수를 자기정보(Self information)라 부르고 $I$라 표기한다.<span style=\"font-size:70%\">[3]</span> $I$는 아래와 같이 일반화가 가능하다.<br><br>\n",
    "$$I=log_{2}{N} = -log_{2}{\\frac{1}{N}}= -log_{2}{p}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1739a2bf",
   "metadata": {},
   "source": [
    "### (3) 메시지의 확률과 정보량의 관계\n",
    "\n",
    "결국 메시지가 갖는 불확실성(Uncerntly)이라는 공통적인 척도로 단일 메시지의 정보량으로 표현하였다는것을 알 수 있다. 그렇다면 메시지의 등장 확률(=불확실성)에 따른 정보량은 어떤 관계를 갖는지 위 수식을 살펴보자\n",
    "\n",
    "$$I= -log_{2}{p}=log_{2}{\\frac{1}{p}}$$\n",
    "\n",
    "즉 발생 확률$p$가 낮아질수록 단일 메시지의 자기 정보량 $I$가 증가한다. 이 컨셉은 꽤나 직관적인데 뉴스 헤드라인에 \"내일은 해가 동쪽으로 뜬다\"라고 뜬다면 별로 관심을 두지 않을것이고 반면 \"일론머스크 사실은 화성인이었음\"이 헤드라인에 뜬다면 모두가 집중할 것이다. 이처럼 각 메시지에 포함된 정보의 양은 차이가 존재하며 발생확률이 낮을수록 메시지가 갖는 정보량이 많다는것이다. 즉 발생확률이 낮을수록 메시지가 갖는 정보량이 크다.<span style=\"font-size:70%\">[4]</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7411e17",
   "metadata": {},
   "source": [
    "### (4) 단일 메시지가 아닌 \"전체 메시지의 일반적인 비트수(bit)\"를 어떻게 계산할 수 있을까? \n",
    "\n",
    "메시지전송 사업은 나날이 번창해 고객이 많아진 만큼 주변 경쟁자들이 많이 생겨났다. 이제는 텍스트로된 메시지를 고객들이 주로 전송하다보니 bit수가 나날이 증가해 고객들의 비용에 대한 불만도 증가했다. 경쟁회사보다 저렴한 가격으로 메시지를 전송할 수 있는 방법을 찾다보니 단어구성에 재밌는 규칙이 존재하는것을 발견했다. 바로 알파벳중에 많이 등장하는 알파벳이 있고 적게 등장하는 알파벳이 있던것이었다.\n",
    "\n",
    "편의상 알파벳이 4개밖에 존재하지 않고 아래와 같은 확률로 등장한다고 생각해보자.\n",
    "\n",
    "    A : 50%\n",
    "    B : 25%\n",
    "    C : 15%\n",
    "    D : 10%\n",
    "\n",
    "<br><br>\n",
    "<div align=\"center\">\n",
    "<img src=\"https://github.com/int29/PMLP-101/blob/main/chapter_98_appendix/img/98_02.png?raw=true\" width=\"500px\">\n",
    "\t<p>[그림98_03]</p>\n",
    "</div>\n",
    "<br><br>\n",
    "\n",
    "가장 적은 전송횟수로 전달하는 방법은 절반에 가깝게 계속 분할하는 것이다. 따라서 A와 BCD를 먼저 나누고 B와 CD를 나누고 종국에는 C와 D를 분할하는 방식으로 메시지 전송하는 방법이 최적의 전송법일 것이다. 결국 알파벳 A만 전송하면 1번만 전송하면 되기 때문에 1bit, B는 2번이기 때문에 2bit C 나 D만을 전송하면 3bit가 필요하게 된다.\n",
    "\n",
    "이후 각 고객별로 어떤 문자를 전송했고 얼마나 전송하기 위한 비트수가 몇이었는지를 모두 기록을 해나갔다. 그 결과 아래와 같은 분포를 얻었게 되었다. \n",
    "\n",
    "<br><br>\n",
    "<div align=\"center\">\n",
    "<img src=\"https://github.com/int29/PMLP-101/blob/main/chapter_98_appendix/img/98_03.png?raw=true\" width=\"750px\">\n",
    "\t<p>[그림98_04]</p>\n",
    "</div>\n",
    "<br><br>\n",
    "\n",
    "보통 고객들은 경우에 따라서 1\\~3 bit를 전송을 위해 필요로 하였지만 이를 평균적으로 계산하면 1.6~1.7정도의 bit를 필요로 한다는 사실을 알아냈다. 정확한 평균 비트값을 계산하기 위해 ABCD등장확률을 통한 기대값을 계산한 결과 1.75의 bit를 전송을 위해 필요로 한다는 사실을 계산해 낼 수 있었다.\n",
    "\n",
    "$$E(X)=0.5 \\cdot 1bit + 0.25 \\cdot 2bit + 0.15 \\cdot 3bit + 0.10 \\cdot 3bit$$ <br><br>\n",
    "$$E(X)=1.75 bit$$\n",
    "\n",
    "결국 모든고객에게 1.75bit를 받으나 개별요금을 받으나 동일한 값을 받게 되기 때문에 글자당 1.75bit를 공통으로 받는 요금제를 출시하게 된다.(물론 약간의 이득이나 손실은 있더라도 장기적으로는 동일해진다.)\n",
    "\n",
    "> 비트수의 기대값은 주사위 눈의 기대값과 동일하게 계산가능하다. 즉 공정한 주사위는 각 눈의 확률이 $\\frac{1}{6}$ 이기 때문에 눈의 기대값 $E(X)$는 아래와 같다. <br><br>\n",
    "$$E(X)=\\frac{1}{6} \\cdot 1 + \\frac{1}{6} \\cdot 2 + \\frac{1}{6} \\cdot 3 + \\frac{1}{6} \\cdot 4 + \\frac{1}{6} \\cdot 5 + \\frac{1}{6} \\cdot 6$$ <br><br>\n",
    "$$E(X)=3.5$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c230df81",
   "metadata": {},
   "source": [
    "### (5) 정보 엔트로피(information entropy)와 머신러닝\n",
    "\n",
    "그동안은 개별 메시지의 등장확률을 통해 각 메시지가 갖는 자기 정보량$I$(비트수)를 계산했었다. 하지만 동전던지기 그 자체와 알파벳조합의 단어를 전달하는 그 자체의 정보량 측정을 하려면 어떻게 해야할까? 바로 메시지의 확률분포를 바탕으로 비트수의 기대값을 계산하여 구할 수 있다. 즉 개별 메시지가 아닌 메시지 자체의 정보량을 측정할 수 있는 것이다.\n",
    "\n",
    "단일 확률에 따른 정보량을 자기정보(self information)으로 $I$로 나타냈다면, 위의 1.75bit의 표준요금제처럼 특정 확률분포를 따르는 메시지의 정보량을 비트수의 기대값으로 계산하 였고, 섀넌은 이를 정보 엔트로피라(information entropy)정의하고 아래와 같이 제안하였다.<br><br>\n",
    "\n",
    ">엔트로피는 무작위성의 정도를 자료가 지니는 확률분포로부터 정량적으로 평가하는 방법이다.(Shannon, 1948)<span style=\"font-size:70%\">[5]</span>\n",
    "\n",
    "$$H(X)=\\sum_{i=1}^{n}-p\\cdot log_{2}{p}$$<br><br>\n",
    "\n",
    "1개의 동전던지기는 경우도 확률변수 $X$는 확률이 50%인 균등분포를 따르기 때문에 아래와 같이 정보 엔트로피$H(X)$(비트수의 기대값)를 구할 수 있다.<br><br>\n",
    "\n",
    "$$E(X)=0.5 \\cdot 1bit + 0.5 \\cdot 1bit$$ <br>\n",
    "$$E(X)=1bit$$<br><br>\n",
    "\n",
    "동전2개 던지기의 경우도 각 사건은 25%인 균등분포로 기대 비트값은 아래와 같다.\n",
    "<br><br>\n",
    "$$E(X)=0.25 \\cdot 2bit + 0.25 \\cdot 2bit + 0.25 \\cdot 2bit + 0.25 \\cdot 2bit$$ <br>\n",
    "$$E(X)=2bit$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4966c80",
   "metadata": {},
   "source": [
    "따라서 결과적으로 측정에 필요한 확률분포만 알 수 있다면 아래와 같이 전화선으로 전달되는 정보량에 대해서 엔트로피로 측정할 수 있고 이를 바탕으로 채널의 용량이나 기타 공학적인 설계에 이용되어 왔다. 또한 확률변수, 정보(=데이터)에 대한 컨셉이 다수 머신러닝과 연결점이 많은 만큼 자연스래 정보이론은 기존 머신러닝과 딥러닝에서 분류와 변수선택에서 많이 사용되어 왔다. (Ahn과 Kim, 2014, 2018)<span style=\"font-size:70%\">[6]</span>\n",
    "\n",
    "> 1분동안 전화선으로 얼마나 많은 정보가 전달되는지 수치화 할 수 있을까?<span style=\"font-size:70%\">[7]</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855c0b50",
   "metadata": {},
   "source": [
    "### (6) 정보 엔트로피와 정보량\n",
    "\n",
    "단일사건의 불확실성(확률)에 따른 정보량을 계산하는 자기 정보(self information) $I$는 발생확률이 낮을수록 더 많은 정보를 갖는 특징이 있었다. 그렇다면 self information의 기대값인 엔트로피 $H(X)=E(I(p))$ 는 정보량은 어떤 관계를 갖을까?\n",
    "\n",
    "먼저 직관적으로 한번 생각을 해보자. 아래 두가지 상황 중 더 예측하기 어려운것은 무엇인가?\n",
    "\n",
    "    상황01 : 동전던지기 앞뒷면 맞추기\n",
    "    상황02 : 99%확률로 비 예보가 있을 경우 비가 올지 안올지 맞추기\n",
    "    \n",
    "대부분은 상황01이 더 어려울것이라고 생각할 것이다. 동전던지기의 경우 대체로 앞뒷면이 나올 확률은 50%로 근사하기 때문에 어떤면이 나올지 예측하기가 어렵기 때문이다. 즉 정보 엔트로피는 아래와 같이 50%일 때 엔트로피가 최대값이 되며 50%에서 감소하거나 증가할 수록 특정 사건이 명확해지는것으로 엔트로피는 감소한다.\n",
    "\n",
    "<br><br>\n",
    "<div align=\"center\">\n",
    "<img src=\"https://github.com/int29/PMLP-101/blob/main/chapter_98_appendix/img/98_04.png?raw=true\" width=\"450px\">\n",
    "\t<p>[그림98_05]</p>\n",
    "</div>\n",
    "<br><br>\n",
    "\n",
    "\n",
    "쉽게 확률이 반반에 가까워 질 수록 예측할 수 없는 혼란스러운 상태이고 그만큼 설명을 위해 표현해야하는 정보량이 늘어나는것으로 생각해봄직하다. 즉 전달해야하는 정보가 많을 수록 정리되지 않은 상태라는 것이다.<span style=\"font-size:70%\">[8]</span>\n",
    "\n",
    "> 간단히 수학적으로 비트표현은 이진 분할 혹은 이항분포의 확률문제와 동일하다. 특정 케이스의 발생확률 $p$가 증가하면 그 반대 확률인 $q=1-p$이기 때문에 자연스래 감소하고 $p$가 감소하면 그 반대확률 $q$ 또한 증가한다. 이러한 이율배반(trade off) 상황에서 기대값이 가장 최대값을 갖는 경우는 각 확률이 동일한 $p=0.5$와 $1-p=0.5$일때이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28984f57",
   "metadata": {},
   "source": [
    "### (7) 지니불순도(Gini impurity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c458c7e",
   "metadata": {},
   "source": [
    "지니 불순도는 집합에 이질적인 것이 얼마나 섞여 있는지를 측정하는 지표로서 분류모델에서 영역을 나누는 기준의 하나이다. 정보 이론적 측면에서는 가중치가 고려된 엔트로피로 해당한다. 0에서1의 값을 가지며 값이 클수록 다른 정보와 이질적임을 의미한다. 지니 불순도는 다음과 같이 계산된다 (Breiman, 1996; Rokach와 Maimon, 2005).<span style=\"font-size:70%\">[9]</span> \n",
    "\n",
    "지니불순도(Gini impurity)를 통한 분할 방법은 **어떤 그룹이 한가지로 구성되어 있을 수록(동질성;homogeneity이 높을수록) 선택하기 쉽다** 는 컨셉을 갖고있다. 이 컨셉은 꾀나 직관적인데 아래그림에서 양끝단으로 갈수록 판단하기에 수월하고 중앙 붉은공 2개 vs 푸른공 2개는 어떤 공주머니 인지 판단할 수 없는 상태임을 확인할 수 있다. 이처럼 지니불순도를 활용한 분할 기준을 활용할 경우 부모노드(Parents node)에서 자식노드(Children nodes)로 분할할 때 지니불순도(gini impurity)를 최대한 낮출수 있는 기준을 활용하는 분할 방법이다.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315f8c9c",
   "metadata": {},
   "source": [
    "\n",
    "<div align=\"center\">\n",
    "\t<img src=\"https://github.com/int29/PMLP-101/blob/main/chapter_02_machine_learning_algorithms/02_02_%EC%9D%98%EC%82%AC%EA%B2%B0%EC%A0%95%EB%82%98%EB%AC%B4(DecisionTree)/img/03_04.png?raw=true\" width=500>\n",
    "    <p>[그림98_06]</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89250129",
   "metadata": {},
   "source": [
    "### 지니(Gini Index)\n",
    "\n",
    "$Gini(D)$는 데이터셋 $D$의 각 클래스가 얼마나 균일하게 존재하는지를 나타내는 지표이고 각 클래스 비율의 제곱합으로 계산할 수 있다.\n",
    "데이터 내 특정 클래스 $i$의 비율을 $p_{i}$ 라고 하고 $C_{i}$를 클래스 $i$의 수, $D$를 데이터셋의 크기로 정의할 때 각 클래스가 차지하는 비율$p$는 아래와 같기 때문에 데이터 내 각 클래스의 비율은 아래와 같이 계산할 수 있다.<span style=\"font-size:70%\">[10]</span>  <br><br>\n",
    "$$p_{i} = \\frac{C_{i}}{D}$$ \n",
    "\n",
    "따라서 지니값은 아래와 같이 계산한다.\n",
    "$$ Gini(D) =(p_{1}^{2}+p_{2}^{2}+p_{3}^{2}+...+p_{k}^{2}) = \\sum_{i=1}^{k}p_{i}^2=\\sum_{i=1}^{k}\\Big(\\frac{C_{i}}{D}\\Big)^2$$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441d5f98",
   "metadata": {},
   "source": [
    "지니값(Gini)은 얼마나 데이터가 균일한지 나타내기 때문에 $Gini(D)$에 1을 뺄 경우 얼마나 불순도가 높은지를 표현할 수 있게된다.<br><br>\n",
    "$$I_{G} = 1 - Gini(D)$$<br><br>\n",
    "따라서 지니불순도(Gini Impurity)는 아래와 같이 표현이 가능하다.\n",
    "$$I_{G} = 1 - (p_{1}^{2}+p_{2}^{2}+p_{3}^{2}+...+p_{k}^{2})= 1 -\\sum_{i=1}^{k}p_{i}^{2}=\\sum_{i=1}^{k}\\Big(\\frac{C_{i}}{D}\\Big)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40677986",
   "metadata": {},
   "source": [
    "이를 통해 주머니 그림의 지니불순도를 계산하면 아래와 같이 계산이 가능하다.\n",
    "\n",
    "<div align=\"center\">\n",
    "\t<img src=\"https://github.com/int29/PMLP-101/blob/main/chapter_02_machine_learning_algorithms/02_02_%EC%9D%98%EC%82%AC%EA%B2%B0%EC%A0%95%EB%82%98%EB%AC%B4(DecisionTree)/img/03_05.png?raw=true\" width=600>\n",
    "    <p>[그림08_07]</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2bb6f9",
   "metadata": {},
   "source": [
    "### (8)지니불순도(Gini Impurity)를 통한 정보이득(Information Gain)\n",
    "\n",
    "<br><br>\n",
    "$$I(D,A)=I_{G}(D)-\\sum_{j=1}^{m}\\frac{D_{j}}{D}I_{G}(D_{j})$$ \n",
    "<br>\n",
    "여기서 $D$는 데이터 세트, $A$는 속성, $D_{j}$ 는 속성 $A$에 따라 분할된 데이터 세트, $I_{G}(D)$는 데이터 세트 $D$의 불순도, $I_{G}(D_{j})$는 데이터 세트 $D_{j}$의 불순도 일 때 정보 이득은 데이터 세트를 속성 $A$로 분할했을 때의 불순도 감소량으로 나타낼 수 있다. <span style=\"font-size:70%\">[10]</span> \n",
    "\n",
    "이를 트리가 이진 분류일 때 아래와 같이 표현할 수 있다.<span style=\"font-size:70%\">[11]</span> \n",
    "$$I(D,A)=I_{G}(D)-\\big(\\frac{D_{left}}{D}I_{G}(D_{left})+\\frac{D_{right}}{D}I_{G}(D_{right})\\big)$$\n",
    "\n",
    "여기서 $\\frac{D_{left}}{D}$는 데이터$D$에서 $D_{left}$로 나눠졌을때의 데이터의 수이다. 즉 데이터의 사이즈를 가중치로 사용하는데 그 이유는 특정 노드의 데이터 수가 많을수록 해당 노드에 대한 예측이 더 정확할 가능성이 높기 때문이다.<span style=\"font-size:70%\">[10]</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91c0829",
   "metadata": {},
   "source": [
    "아래 에서 부모노드의 $I_{G}(D)$는 0.667이고 $I_{G}(D_{left})$는 0  $I_{G}(D_{right})$는 0.5이다. 좌측(left)노드의 샘플수($D_{left}$)는 50, 우측(right)노드의 샘플수($D_{right}$)는 100이므로\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "\t<img src=\"https://github.com/int29/PMLP-101/blob/main/chapter_02_machine_learning_algorithms/02_02_%EC%9D%98%EC%82%AC%EA%B2%B0%EC%A0%95%EB%82%98%EB%AC%B4(DecisionTree)/img/03_06.png?raw=true\" width=500>\n",
    "    <p>[그림03_06]</p>\n",
    "</div>\n",
    "\n",
    "정보이득은 아래와 같이 0.334로 계산할 수 있다.<br><br>\n",
    "$$I(D,A)=I_{G}(D)-\\big(\\frac{D_{left}}{D}I_{G}(D_{left})+\\frac{D_{right}}{D}I_{G}(D_{right})\\big)$$<br>\n",
    "$$=0.667-(\\frac{50}{150}\\cdot0 + \\frac{100}{150}\\cdot0.5)$$<br>\n",
    "$$=0.334$$\n",
    "\n",
    "이와 동일하게 모든 가능한 속성 $A_{j}$를 계산할 때 0.334 값이 최고의 정보이득값으로 판명하였기 때문에 의사결정나무 알고리즘은 데이터 $D$를 속성 **Petal length (cm)를 2.5 이하**로 분할하고 이 과정을 계속 반복한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49dd663",
   "metadata": {},
   "source": [
    "#### 미주\n",
    "1 :엔트로피에 대한 예시 및 설명은 칸 아카데미(khanacademy)의 Computer science 강좌의 Information entropy 챕터를 대부분 인용하였으며 예시와 설명의 경우 상황에 맞게 약간의 수정 후 재구성하였을 뿐임을 밝힘. (https://www.khanacademy.org/computing/computer-science/informationtheory/moderninfotheory/v/information-entropy)\n",
    "\n",
    "#### 인용출처\n",
    "\n",
    "[1] 정보와 통신 : 한국통신학회지 = Information & communications magazine v.25 no.5 , 2008년, 김진영,김윤현,허준, 5page, 요약과 서론을 인용하여 재구성함\n",
    "> 원문 : \"정보 이론은 최대한 많은 데이터를 매체에 저장하거나 채널을 통해 통신하기 위해 데이터를 정량화하는 응용 수학의 한 분야이다.\"과 \"즉 정보이론은 메시지 신호에 포함된 정보의 정량적 측정량을 의미하며...<후략>\"\n",
    "\n",
    "[2] 머신러닝과 정보이론: 작동원리의 이해(2021),조정효,과학의지평,고등과학원 블로그에서 인용후 재구성함(https://horizon.kias.re.kr/18474/)\n",
    "> 원문 : \"정보량을 수치화 할 수 있을까? 1948년 벨 연구소의 클로드 섀넌Claude Shannon은 “A mathematical theory of communication”이라는 제목의 논문을 발표한다.\"\n",
    "\n",
    "[3] Shannon의 정보이론과 문헌정보,도서관학 = Journal of the Korean Library Science Society v.6 , 1979년, 정영미, 90page, 2.1 Shannon의 정보이론에서 인용하여 재구성함.\n",
    "> 원문 : \"Shannon의 정보이론에서 사용한 정보란 말의 개념은 일반적인 커뮤니케이션 이론에서 말하는 정보의 개념과는 다르다. 공학적으로 해석되는 전보는 메시지 내용이 뜻하는 바와는 무관하다. 텔레커뮤니케이션 채널을 통해 전송되거나 컴퓨터내에서 처리되는 메시지는 자연어로 된것이 아니라 코우딩 된 형태의 것이다.\"과 \"결국 정보이론에서 말하는 정보량은 실질적으로는 메시지를 이진수 (0과 1) 로 코우딩할 때 필요한 최소한의 자리수에 해당한다.\"\n",
    "\n",
    "[4] 정보와 통신 : 한국통신학회지 = Information & communications magazine v.25 no.5 , 2008년, 김진영,김윤현,허준, 6page, II.정보 를 인용하여 재구성함\n",
    "> 원문 : \"다음과 같은 조간 신문의 세가지 머릿기사를 생각해보자 1. 내일은 해가 동쪽에서 뜰것이다. 2. 미국이 쿠바를 침공했다. 3. 쿠마가 미국을 침공했다. <중략> 예상치못했던사건일수록 놀라움은 더 커지게 되며, 따라서 더 많은 정보를 내포한다.\"\n",
    "\n",
    "[5] 엔트로피와 지니 불순도를 이용한 강우 관측망 설계: 강원도 지역을 중심으로(2020),권태용,윤상후 중 571page, 2.2. 엔트로피 인용\n",
    "> 원문 : \"엔트로피는 무작위성의 정도를 자료가 지니는 확률분포로부터 정량적으로 평가하는 방법이다 (Shan- non, 1948).\"\n",
    "\n",
    "[6] 엔트로피와 지니 불순도를 이용한 강우 관측망 설계: 강원도 지역을 중심으로(2020),권태용,윤상후 중 571page, 2.1. 서론 인용\n",
    "> 원문 : \"정보이론은 분류와 변수선택에서 많이 사용되어 왔다 (Ahn과 Kim, 2014, 2018).\"\n",
    "\n",
    "[7] 소프트웨어 세상, 컴퓨터의 스무고개, 섀넌의 정보이론(2016),EBS, 01:15초 인용 : https://www.youtube.com/watch?v=2LWknQ-FkGI \n",
    "\n",
    "[8] 머신러닝에서 딥러닝까지(),곽동민, 박세원, 이한남(2015), 260page 인용\n",
    "> 원문 : Entropy는 bit의 expected value라고 할 수 있다. 전달해야하는 정보의 양이 늘어날수록 정보가 덜 정리되어 있다고 볼 수 있고 이는 thermodynamics에서 이야기하는 entropy가  chaotic한 정도라는 정의와 상통한다는 것을 알 수 있다. \n",
    "\n",
    "[9] 엔트로피와 지니 불순도를 이용한 강우 관측망 설계: 강원도 지역을 중심으로(2020),권태용,윤상후 중 571page, 2.3. 지니불순도 인용\n",
    "> 원문 : \"지니불순도는 집합에 이질적인 것이 얼마나 섞여있는지를 측정하는 지표로서 <중략> 정보 이론적 측면에서는 가중치가 고려된 엔트로피이다.\"\n",
    "\n",
    "[10] 구글 바드를 통해 수식을 생성하고 수식설명을 인용 및 재구성함.(https://bard.google.com/)<br>\n",
    "\n",
    "[11] 텐서플로우 블로그의 수식인용 : https://tensorflow.blog/tag/gini-impurity/\n",
    "\n",
    "#### 참고문헌\n",
    "1. 정보와 통신 : 한국통신학회지 = Information & communications magazine v.25 no.5 , 2008년, 김진영,김윤현,허준\n",
    "2. Shannon의 정보이론과 문헌정보,도서관학 = Journal of the Korean Library Science Society v.6 , 1979년, 정영미\n",
    "3. A Mathematical Theory of Communication(1948), C. E. SHANNON\n",
    "4. 머신러닝에서 딥러닝까지(),곽동민, 박세원, 이한남(2015)\n",
    "\n",
    "#### 이미지 출처\n",
    "\n",
    "[그림98_01] : 내일은 전보 개통 131년…'생활속 전보 이야기' 공모, 아시아경제 (https://www.asiae.co.kr/article/2016092716332909423&mobile=Y)\n",
    "[그림98_05] : A Mathematical Theory of Communication(1948), C. E. SHANNON, 11page, Fig. 7— Entropy in the case of two possibilities with probabilities p and (1 - p).\n",
    "\n",
    "#### 참고 웹페이지\n",
    "\n",
    "1. 텐서플로우 블로그 : https://tensorflow.blog/tag/gini-impurity/\n",
    "2. 머신러닝과 정보이론: 작동원리의 이해(2021),조정효,과학의지평,고등과학원 블로그:https://horizon.kias.re.kr/18474/\n",
    "3. 칸 아카데미(khanacademy)의 Computer science 강좌 Information entropy를 인용하여 재구성함(https://www.khanacademy.org/computing/computer-science/informationtheory/moderninfotheory/v/information-entropy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
