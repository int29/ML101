{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edec78ff",
   "metadata": {},
   "source": [
    "## 98. 머신러닝을 위한 정보이론(Information Theory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb31ccc",
   "metadata": {},
   "source": [
    "우리가 신호를 송신해주고 돈을 받는 사업을하고 있다고 생각해보자 우리의 고객들은 비용을 지불하여 다양한 정보를 송신하고 싶어한다. 1900년대다 보니 지금처럼 스마트폰이 있는 세계는 아니고 모스기계를 통해 메시지를 상대방에게 전달한다.\n",
    "\n",
    "모스신호기는 0과 1로만 신호를 표현하여 전달한다.\n",
    "<img src=\"https://cphoto.asiae.co.kr/listimglink/1/2016092716332909423_1.jpg\" width=\"300px\">\n",
    "\n",
    "우리에게 고객 3명이 찾아왔다. 고객 A는 동전던지기 결과를 전달하고 싶어하고, 고객 B는 주사위 내기를 위해 주사위의 결과를 전달하고 싶어한다. 고객 A, B 에게 각각 얼마를 받아야할까? 싯가라고 별 기준없이 즉흥적으로 비용을 받으면 장사의 신용은 떨어지고 경쟁업체가 득세할 것이다.\n",
    "\n",
    "우리는 고기를 구매하던 생선을 구매하던지 kg이라는 기준을 통해 물품의 가격을 측정하고 이 기준을 통해 산정된 결과를 지불한다. 이처럼 정보 또한 하나의 물리량으로 측정하여 전달하는것이 합리적일 것이다. \n",
    "\n",
    "정보 이론은 1948년 벨 연구소의 클로드 섀넌(Claude Shannon)이 “A mathematical theory of communication”이라는 논문을 발표하여 시작된 이론으로 엔트로피라는 개념을 통해 메시지의 정보량을 정량적으로 측정하는 방법에 대해 다루는 이론이다.[1] 즉 킬로그램(Kg)이나, 리터(L)처럼 메시지속 정보의 양을 정보 엔트로피라는 수치로 계산하는 방법에 대해 연구하는 응용수학인 것이다. \n",
    "\n",
    "> 정보의 가치를 그저 감성적으로 이해하던 것을 1945년경 미국의 섀넌(Shannon)은 정보의 가치를 엔트로피(entropy)라는 것으로 계량화하였다. 어떤 정보의 엔트로피란 주어진 정보를 될 수 있는 데로 압축하여 0과 1로 표현할 때 필요한 평균 비트 수이다. 엔트로피를 통하여 정보를 질량이나 에너지, 운동량과 같은 수량화된 물리량으로 만들었다. [10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8c3fa5",
   "metadata": {},
   "source": [
    "### (1)  메시지의 이진수 표현 \n",
    "\n",
    "뭐가 됐던지 간에 고객 A,B가 전송하고자 하는 정보의 양을 일단 0과 1로 전달할 수 있도록 변경을 해야한다. 불가능하다면 전송 자체가 안되기 때문이다.\n",
    "\n",
    "고객 A부터 시작해보자 \n",
    "\n",
    "    고객A : 동전던지기 내기 결과를 전송하여 돈을 청구하고 싶습니다.\n",
    "\n",
    "고객 A는 동전던지기의 결과를 전달하기를 원한다. 동전던지기는 나올 수 있는 경우의수는 앞면과 뒷면 두가지 뿐이고 이를 0과 1로 표현하면 아래와 같이 표현할 수 있을것이다.\n",
    "\n",
    "    [앞]: 0\n",
    "    [뒤]: 1\n",
    "\n",
    "고객 B는 주사위 결과를 전달하기를 원한다. 주사위 결과는 동전던지기와 비슷하지만 경우의 수가 더 많기 때문에 조금 더 복잡해졌다. 한번 0과1을 내맘대로 조합해서 표현해보자\n",
    "\n",
    "    고객B : 주사위를 던져 나온 결과를 전달해 내기결과를 전달하고 싶습니다.\n",
    "\n",
    "    [1]: 000\n",
    "    [2]: 010\n",
    "    [3]: 100\n",
    "    [4]: 110\n",
    "    [5]: 001\n",
    "    [6]: 011"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f845c9",
   "metadata": {},
   "source": [
    "### 신호 전송횟수 : 비트(bit)\n",
    "이제 한번 기준을 생각해보자, 0과1로만 전달해야하는데 고객A는 어떤 결과가 나오던지간에 한번의 전송으로 표현이 가능하다. 앞면이 나왔다면 0, 뒷면이 나왔다면 1을 전송하면 되는것이다. 반면에 고객 B는 적어도 3번의 전송이 필요하다.\n",
    "\n",
    "우리는 이렇게 신호를 전달하는 횟수를 비트라 부르기로 했고 Kg처럼 비트를 통해 요금을 산정하기로 정한다면 사업자인 우리와 비용을 지불하는 고객들은 별 불만이 없을 것이다.\n",
    "\n",
    "    <고객 A> \n",
    "    * 1 회 전송 = 1bit\n",
    "    [앞]: 0\n",
    "    [뒤]: 1\n",
    "    \n",
    " \n",
    "    <고객 B> \n",
    "    * 3 회 전송 = 3bit\n",
    "    [1]: 000\n",
    "    [2]: 001\n",
    "    [3]: 010\n",
    "    [4]: 011\n",
    "    [5]: 100\n",
    "    [6]: 101"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cdf38e",
   "metadata": {},
   "source": [
    "모든 경우의 수를 2진수로 표현하기 때문에 동일한 정보를 아래 이진트리로 표현하면 더 직관적인데, 이를 통해 우리가 정보를 전달하기 위해 필요한 전송횟수, 즉 비트수는 $2^{n}$=전송하는 경우의 수로 나타낼수 있는 알 수 있다. 즉 $n$을 구하면 몇번을 통해 전송해야 하는지 bit로 계산할 수 있는 것이다.\n",
    "\n",
    "만약에 26개로 이루어진 알파벳 한글자를 전송하기 위해 필요한 전송횟수(비트수)는 $2^{n}=26$로 표현할 수 있고 n을 구하면 알파벳 한글자를 전송하는데 필요한 비트수를 계산할 수 있게된다. 우리는 $n$값을 계산하는 쉬운 방법을 우리는 알고 있다. $log_{2}{4}=2$ 라는 의미는 2를 몇 제곱해야지 4가 되는지에 대한 표기이다. 즉 $2^{n}=26$ 는 $log_{2}{26}=n$ 으로 표현할 수 있고 $log_{2}{26}=4.7$ 정도 이기 때문에 필요한 비트수는 약 4.7로 최소 5번전송이 필요하게 된다.(0.7번 전송은 없기 때문에 5번 전송이 필요하고 5비트가 된다.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59f6062",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfc1522",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4966c80",
   "metadata": {},
   "source": [
    "정보 이론은 1948년 벨 연구소의 클로드 섀넌(Claude Shannon)이 “A mathematical theory of communication”이라는 논문을 발표하여 시작된 이론으로 엔트로피라는 개념을 통해 메시지의 정보량을 정량적으로 측정하는 방법에 대해 다루는 이론이다.[1] \n",
    "\n",
    "> 전화선으로 얼마나 많은 정보가 전달되는지 수치화 할 수 있을까?[9]\n",
    "\n",
    "> 정보의 가치를 그저 감성적으로 이해하던 것을 1945년경 미국의 섀넌(Shannon)은 정보의 가치를 엔트로피(entropy)라는 것으로 계량화하였다. 어떤 정보의 엔트로피란 주어진 정보를 될 수 있는 데로 압축하여 0과 1로 표현할 때 필요한 비트 수이다. 엔트로피를 통하여 정보를 질량이나 에너지, 운동량과 같은 수량화된 물리량으로 만들었다. [10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3df4415",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef7ce540",
   "metadata": {},
   "source": [
    "\n",
    "말은 어렵지만 담고있는 의미는 직관적이다. 뉴스 헤드라인에 \"내일은 해가 동쪽으로 뜬다\"라고 뜬다면 별로 관심을 두지 않을것이고 반면 \"일론머스크 사실은 화성인이었음\"이 헤드라인에 뜬다면 모두가 집중할 것이다. 이처럼 각 메시지에 포함된 정보의 양은 차이가 존재하며 발생확률이 낮을수록 메시지가 갖는 정보량이 많다는것이다. 즉 발생확률이 낮을수록 메시지가 갖는 정보량이 크다는 것이다.$^{(1)}$\n",
    "\n",
    "결론적으로 정보이론에서 말하는 정보와 정보량이란 0과 1의 이진수로 변환한 부호화된(코딩된) 신호인 비트를 의미하고[] 이렇게 부호화된 정보를 \"발생확률을 고려하여 평균비트값(비트값의 기대값)으로 정량화한 정보 엔트로피(Information Entropy)\"를 통해 측정된 것이 정보량이라는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb05711e",
   "metadata": {},
   "source": [
    "> 정보이론은 분류와 변수선택에서 많이 사용되어 왔다 (Ahn과 Kim, 2014, 2018)[]\n",
    "\n",
    "머신러닝이나 딥러닝에서 에서 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811d3c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "메시지의 불확실성을 0과1의 이진수로 변환했을 때 필요한 최소자리수, 즉 불확실성 표현을 위한 비트수를 의미한다. 정리하면 정보이론은 메시지를 비트로 변경할때 필요한 비트수를 측정하는 방법에 대한 다루는 이론이다.[2]\n",
    "\n",
    "이러한 정보량을 측정하기 위해 정보이론에서는 정보엔트로피(information entropy) 라는 개념을 통해 정보의 양을 측정한다. 엔트로피는 한계 엔트로피, 조건 엔트로피(지니 불순도), 결합엔트로피로 정보의 양을 수치적으로 정량화한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ed9407",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d941e4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc3eca3",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fb1d35",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f06adb80",
   "metadata": {},
   "source": [
    "### (1) 비트표현과 필요 비트수 그리고 확률과의 관계\n",
    "특정 정보를 0과 1로만 2진수로 표현할 수 있는데, 이를 비트 표현방법이라고 한다. 예를 들어 동전의 앞면을 0 뒷면을 1로 정하고 동전던지기의 결과를 전달한다고 해보자. 따라서 앞이나오면 0을 뒤가 나오면 1로만 전달하면 되기 때문에 0,1로만 모든 결과를 빠짐없이 전달가능하다. 따라서 동전 1개의 결과를 이진수로 표현하기 위한 최소자리수는 1로 1 bit이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0487a62b",
   "metadata": {},
   "source": [
    "    [앞]: 0\n",
    "    [뒤]: 1\n",
    "    \n",
    "    >> 1 bit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e86a9f",
   "metadata": {},
   "source": [
    "그렇다면 공정한 동전 2개를 던질경우 어떻게 표현이 가능할까? 나올 수 있는 모든 사건의 경우의 수를 0과 1로 표현해보면 아래와 같이 4가지=$2^2$로 표현이 가능하고 4가지 사건을 모두 0과1로 설명하기 위해서 필요한 최소자리수는 2자리이다. 즉 2 bit가 필요한것을 알 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029a2627",
   "metadata": {},
   "source": [
    "    [앞,앞]: 11\n",
    "    [앞,뒤]: 10\n",
    "    [뒤,앞]: 01\n",
    "    [뒤,뒤]: 11\n",
    "    \n",
    "    >> 2 bit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac47836",
   "metadata": {},
   "source": [
    "즉, 어떤 사건이나 메시지를 비트로 표현 하기 위해 필요한 비트수는 모든 경우의 수를 $2^{n}$으로 표현할 때, $n$이 bit수가 되는것이다. 확률 또한 모든 경우의수 중 해당 사건의 발생빈도이기 때문에 결국 특정 메시지의 확률($p$)를 통해 비트수를 아래와 같이 계산 가능하다.\n",
    "\n",
    "$$-log_{2}{p}$$ \n",
    "\n",
    ">왜 로그를 취하는지 의문점이 들 수 있는데 $log_{2}{4}=2$ 라는 의미는 2를 몇 제곱해야지 4가 되는지에 대한 표기이기 때문에 답은 자연스래 2가된다. 즉 $log_{2}{p}$는 2를 몇 제곱해야지 $p$가 나오는지를 구하는 $log_{2}{p}=2^{n}=p$인 $n$를 구하는 것과 동일한 계산이다. 또 확률은 분수이고 로그를 통해 분수를 계산할 경우 음수가 자연스럽게 나오기 때문에 -값을 취해 비트를 양의 값으로 변환해 주기 위한 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf201c6",
   "metadata": {},
   "source": [
    "결국 특정 사건을 비트로 표현한다는 말은 어떤 사건의 확률, 즉 불확실성을 비트로 표현하는것과 같은 것이다. 따라서 희소할수록 확률 $p$에서 분모인 전체 경우의 수$N$은 커지게 될것이고 자연스래 비트로 표현하기 위해서는 많은 자리수가 필요할 것이고 이는 큰 비트수 즉 많은 정보량을 의미하게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198a5ded",
   "metadata": {},
   "source": [
    "### 정보량과 정보 엔트로피(Information Entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea970dee",
   "metadata": {},
   "source": [
    "그렇다면  동전 1개를 4번 던졌을때, 이 나온 결과를 메시지로 전달한다고 하면 이 메시지의 정보량은 몇으로 계산할 수 있을까?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d31994",
   "metadata": {},
   "source": [
    "예를 들어 \"앞뒤앞앞\"에서 각 면이 나올 확률이 $\\frac{1}{2}$이기 때문에 각 개별 사건을 비트로 표현하면 $-log_{2}{\\frac{1}{2}}=1bit$ 가 된다. \n",
    "\n",
    "    [앞]: 1, 1bit >> [뒤]: 0, 1bit >> [앞]: 1, 1bit >> [앞]: 1, 1bit   \n",
    "\n",
    "이 때 기대 비트수는 아래와 같이 계산할 수 있기 때문에\n",
    "$$E(X)=\\frac{1}{2} \\cdot 1bit + \\frac{1}{2} \\cdot 1bit + \\frac{1}{2} \\cdot 1bit + \\frac{1}{2} \\cdot 1bit$$ <br><br>\n",
    "$$E(X)=0.5 + 0.5 + 0.5 +0.5 = 2bit$$\n",
    "\n",
    "\"앞뒤앞앞\" 4번의 동전던지그는 정보량을 2bit라고 측정할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb488edb",
   "metadata": {},
   "source": [
    "$$H(X)=-p(x_{n}) log_{2}{p}(x_{n})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dde252",
   "metadata": {},
   "source": [
    "섀넌은 정보 엔트로피(information entropy)라는 개념을 통해 확률에 따른 비트수의 기대값으로 정보량을 표현하였다.\n",
    "\n",
    "엔트로피는 불확실성을 자료가 지니는 확률분포로부터 정량적으로 평가하는 방법이다 (Shannon, 1948). 한계 엔트로피는 이산확률변수$X$가 갖는 불확실성의 양이며 다음과 같다.[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a30aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "엔트로피는 무작위성의 정도를 자료가 지니는 확률분포로부터 정량적으로 평가하는 방법이다 (Shannon, 1948). 엔트로피는 사용목적에 따라 한계 엔트로피, 조건 엔트로피, 결합엔트로피로 정보의 불확실성을 정량화 한다. Shannon (1948)이 제안한 한계 엔트로피는 이산확률변수$X$가 갖는 불확실성의 양이며 다음과 같다.[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b9b7aa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41f96f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9b6a99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10dc9941",
   "metadata": {},
   "source": [
    "### 01. 엔트로피(Entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855c0b50",
   "metadata": {},
   "source": [
    "엔트로피는 무작위성의 정도를 자료가 지니는 확률분포로부터 정량적으로 평가하는 방법이다 (Shannon, 1948). 엔트로피는 사용목적에 따라 한계 엔트로피, 조건 엔트로피, 결합엔트로피로 정보의 불확실성을 정량화 한다. Shannon (1948)이 제안한 한계 엔트로피는 이산확률변수$X$가 갖는 불확실성의 양이며 다음과 같다.[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28984f57",
   "metadata": {},
   "source": [
    "### 01. 지니불순도(Gini impurity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c458c7e",
   "metadata": {},
   "source": [
    "지니 불순도는 집합에 이질적인 것이 얼마나 섞여 있는지를 측정하는 지표로서 분류모델에서 영역을 나누는 기준의 하나이다. 정보 이론적 측면에서는 가중치가 고려된 엔트로피로 해당한다. 0에서1의 값을 가지며 값이 클수록 다른 정보와 이질적임을 의미한다. 지니 불순도는 다음과 같이 계산된다 (Breiman, 1996; Rokach와 Maimon, 2005).<span style=\"font-size:70%\">[3]</span> \n",
    "\n",
    "지니불순도(Gini impurity)를 통한 분할 방법은 `어떤 그룹이 한가지로 구성되어 있을 수록(동질성;homogeneity이 높을수록) 선택하기 쉽다` 는 컨셉을 갖고있다. 이 컨셉은 꾀나 직관적인데 아래그림에서 양끝단으로 갈수록 판단하기에 수월하고 중앙 붉은공 2개 vs 푸른공 2개는 어떤 공주머니 인지 판단할 수 없는 상태임을 확인할 수 있다. 이처럼 지니불순도를 활용한 분할 기준을 활용할 경우 부모노드(Parents node)에서 자식노드(Children nodes)로 분할할 때 지니불순도(gini impurity)를 최대한 낮출수 있는 기준을 활용하는 분할 방법이다.<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315f8c9c",
   "metadata": {},
   "source": [
    "\n",
    "<div align=\"center\">\n",
    "\t<img src=\"https://github.com/int29/PMLP-101/blob/main/chapter_02_machine_learning_algorithms/02_02_%EC%9D%98%EC%82%AC%EA%B2%B0%EC%A0%95%EB%82%98%EB%AC%B4(DecisionTree)/img/03_04.png?raw=true\" width=500>\n",
    "    <p>[그림98_01]</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89250129",
   "metadata": {},
   "source": [
    "### 지니(Gini Index)\n",
    "\n",
    "$Gini(D)$는 데이터셋 $D$의 각 클래스가 얼마나 균일하게 존재하는지를 나타내는 지표이고 각 클래스 비율의 제곱합으로 계산할 수 있다.\n",
    "데이터 내 특정 클래스 $i$의 비율을 $p_{i}$ 라고 하고 $C_{i}$를 클래스 $i$의 수, $D$를 데이터셋의 크기로 정의할 때 각 클래스가 차지하는 비율$p$는 아래와 같기 때문에 데이터 내 각 클래스의 비율은 아래와 같이 계산할 수 있다. <br><br>\n",
    "$$p_{i} = \\frac{C_{i}}{D}$$ \n",
    "\n",
    "따라서 지니값은 아래와 같이 계산한다.\n",
    "$$ Gini(D) =(p_{1}^{2}+p_{2}^{2}+p_{3}^{2}+...+p_{k}^{2}) = \\sum_{i=1}^{k}p_{i}^2=\\sum_{i=1}^{k}\\Big(\\frac{C_{i}}{D}\\Big)^2$$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c137aea3",
   "metadata": {},
   "source": [
    "### 지니 불순도(Gini Impurity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441d5f98",
   "metadata": {},
   "source": [
    "지니값(Gini)은 얼마나 데이터가 균일한지 나타내기 때문에 $Gini(D)$에 1을 뺄 경우 얼마나 불순도가 높은지를 표현할 수 있게된다.<br><br>\n",
    "$$I_{G} = 1 - Gini(D)$$<br><br>\n",
    "따라서 지니불순도(Gini Impurity)는 아래와 같이 표현이 가능하다.\n",
    "$$I_{G} = 1 - (p_{1}^{2}+p_{2}^{2}+p_{3}^{2}+...+p_{k}^{2})= 1 -\\sum_{i=1}^{k}p_{i}^{2}=\\sum_{i=1}^{k}\\Big(\\frac{C_{i}}{D}\\Big)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40677986",
   "metadata": {},
   "source": [
    "이를 통해 주머니 그림의 지니불순도를 계산하면 아래와 같이 계산이 가능하다.\n",
    "\n",
    "<div align=\"center\">\n",
    "\t<img src=\"https://github.com/int29/PMLP-101/blob/main/chapter_02_machine_learning_algorithms/02_02_%EC%9D%98%EC%82%AC%EA%B2%B0%EC%A0%95%EB%82%98%EB%AC%B4(DecisionTree)/img/03_05.png?raw=true\" width=600>\n",
    "    <p>[그림03_07]</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2bb6f9",
   "metadata": {},
   "source": [
    "### 지니불순도(Gini Impurity)를 통한 정보이득(Information Gain)\n",
    "\n",
    "<br><br>\n",
    "$$I(D,A)=I_{G}(D)-\\sum_{j=1}^{m}\\frac{D_{j}}{D}I_{G}(D_{j})$$ \n",
    "<br>\n",
    "여기서 $D$는 데이터 세트, $A$는 속성, $D_{j}$ 는 속성 $A$에 따라 분할된 데이터 세트, $I_{G}(D)$는 데이터 세트 $D$의 불순도, $I_{G}(D_{j})$는 데이터 세트 $D_{j}$의 불순도 일 때 정보 이득은 데이터 세트를 속성 $A$로 분할했을 때의 불순도 감소량으로 나타낼 수 있다. <span style=\"font-size:70%\">[6]</span> \n",
    "\n",
    "이를 트리가 이진 분류일 때 아래와 같이 표현할 수 있다.<span style=\"font-size:70%\">[7]</span> \n",
    "$$I(D,A)=I_{G}(D)-\\big(\\frac{D_{left}}{D}I_{G}(D_{left})+\\frac{D_{right}}{D}I_{G}(D_{right})\\big)$$\n",
    "\n",
    "여기서 $\\frac{D_{left}}{D}$는 데이터$D$에서 $D_{left}$로 나눠졌을때의 데이터의 수이다. 즉 데이터의 사이즈를 가중치로 사용하는데 그 이유는 특정 노드의 데이터 수가 많을수록 해당 노드에 대한 예측이 더 정확할 가능성이 높기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91c0829",
   "metadata": {},
   "source": [
    "아래 에서 부모노드의 $I_{G}(D)$는 0.667이고 $I_{G}(D_{left})$는 0  $I_{G}(D_{right})$는 0.5이다. 좌측(left)노드의 샘플수($D_{left}$)는 50, 우측(right)노드의 샘플수($D_{right}$)는 100이므로\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "\t<img src=\"https://github.com/int29/PMLP-101/blob/main/chapter_02_machine_learning_algorithms/02_02_%EC%9D%98%EC%82%AC%EA%B2%B0%EC%A0%95%EB%82%98%EB%AC%B4(DecisionTree)/img/03_06.png?raw=true\" width=500>\n",
    "    <p>[그림03_06]</p>\n",
    "</div>\n",
    "\n",
    "정보이득은 아래와 같이 0.334로 계산할 수 있다.<br><br>\n",
    "$$I(D,A)=I_{G}(D)-\\big(\\frac{D_{left}}{D}I_{G}(D_{left})+\\frac{D_{right}}{D}I_{G}(D_{right})\\big)$$<br>\n",
    "$$=0.667-(\\frac{50}{150}\\cdot0 + \\frac{100}{150}\\cdot0.5)$$<br>\n",
    "$$=0.334$$\n",
    "\n",
    "이와 동일하게 모든 가능한 속성 $A_{j}$를 계산할 때 0.334 값이 최고의 정보이득값으로 판명하였기 때문에 의사결정나무 알고리즘은 데이터 $D$를 속성 `Petal length (cm)를 2.5 이하`로 분할하고 이 과정을 계속 반복한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49dd663",
   "metadata": {},
   "source": [
    "#### 인용출처\n",
    "\n",
    "[1] 정보와 통신 : 한국통신학회지 = Information & communications magazine v.25 no.5 , 2008년, 김진영,김윤현,허준, 5page, 요약과 서론을 인용하여 재구성함\n",
    "> 원문 : \"정보 이론은 최대한 많은 데이터를 매체에 저장하거나 채널을 통해 통신하기 위해 데이터를 정량화하는 응용 수학의 한 분야이다.\"과 \"즉 정보이론은 메시지 신호에 포함된 정보의 정량적 측정량을 의미하며...<후략>\"\n",
    "\n",
    "[2] 머신러닝과 정보이론: 작동원리의 이해(2021),조정효,과학의지평,고등과학원 블로그에서 인용후 재구성함(https://horizon.kias.re.kr/18474/)\n",
    "> 원문 : 정보량을 수치화 할 수 있을까? 1948년 벨 연구소의 클로드 섀넌Claude Shannon은 “A mathematical theory of communication”이라는 제목의 논문을 발표한다.\n",
    "\n",
    "[3] Shannon의 정보이론과 문헌정보,도서관학 = Journal of the Korean Library Science Society v.6 , 1979년, 정영미, 90page, 2.1 Shannon의 정보이론에서 인용하여 재구성함.\n",
    "> 원문 : \"Shannon의 정보이론에서 사용한 정보란 말의 개념은 일반적인 커뮤니케이션 이론에서 말하는 정보의 개념과는 다르다. 공학적으로 해석되는 전보는 메시지 내용이 뜻하는 바와는 무관하다. 텔레커뮤니케이션 채널을 통해 전송되거나 컴퓨터내에서 처리되는 메시지는 자연어로 된것이 아니라 코우딩 된 형태의 것이다.\"과 \"결국 정보이론에서 말하는 정보량은 실질적으로는 메시지를 이진수 (0과 1) 로 코우딩할 때 필요한 최소한의 자리수에 해당한다.\"\n",
    "\n",
    "[4] 엔트로피와 지니 불순도를 이용한 강우 관측망 설계: 강원도 지역을 중심으로(2020),권태용,윤상후 중 571page, 2.3. 지니불순도 인용\n",
    "\n",
    "\n",
    "\n",
    "[6] 구글 바드를 통해 수식을 생성하고 수식설명을 인용 및 재구성함.(https://bard.google.com/)<br>\n",
    "> 원문 : 여기서 $D$는 데이터 세트, $A$는 속성, $D_{j}$는 속성 $A$에 따라 분할된 데이터 세트, $H(D)$는 데이터 세트 $D$의 불순도, $H(D_{j})$는 데이터 세트 $D_{j}$의 불순도입니다. 즉, 정보 이득은 데이터 세트를 속성 A로 분할했을 때의 불순도 감소량입니다.\n",
    "\n",
    "[7] 텐서플로우 블로그 : https://tensorflow.blog/tag/gini-impurity/\n",
    "[8] 머신러닝과 정보이론: 작동원리의 이해(2021),조정효,과학의지평,고등과학원 블로그:https://horizon.kias.re.kr/18474/\n",
    "[9] 소프트웨어 세상, 컴퓨터의 스무고개, 섀넌의 정보이론(2016),EBS, 01:15초 인용 : https://www.youtube.com/watch?v=2LWknQ-FkGI \n",
    "\n",
    "[11] 한글의 정보처리 및 통신용 부호 최적화를 위한 한국어 분석(2015), 홍완표 373page 인용\n",
    "\n",
    "\n",
    "#### 참고문헌\n",
    "1. 정보와 통신 : 한국통신학회지 = Information & communications magazine v.25 no.5 , 2008년, 김진영,김윤현,허준\n",
    "2. Shannon의 정보이론과 문헌정보,도서관학 = Journal of the Korean Library Science Society v.6 , 1979년, 정영미\n",
    "3. \n",
    "\n",
    "\n",
    "1. R을 활용한 기계학습, 브레드란츠, 전철욱, 에이콘\n",
    "2. 자료구조(2018),조행래,영남대학교,K-Mooc (http://www.kmooc.kr/courses/course-v1:YeungnamUnivK+YU216002+2018_01/about)<br>\n",
    "3. 이산수학(2016),원성현,부산가톨릭대학교,KOCW http://www.kocw.net/home/cview.do?cid=2df3e8cf09399ca3\n",
    "4. 파이썬을 활용한 머신러닝: 사이킷런 핵심 개발자가 쓴 머신러닝과 데이터 과학 실무서, \n",
    "5. 머신러닝에서 딥러닝까지\n",
    "\n",
    "\n",
    "#### 이미지 출처\n",
    "\n",
    "[그림03_01],[그림03_02] : 의사결정나무 차트(https://scikit-learn.org/stable/modules/tree.html#tree)\n",
    "\n",
    "[그림03_07] : 지니 불순도 차트(https://tensorflow.blog/tag/gini-impurity/)\n",
    "\n",
    "#### 참고 웹페이지\n",
    "\n",
    "1. 텐서플로우 블로그 : https://tensorflow.blog/tag/gini-impurity/\n",
    "2. 머신러닝과 정보이론: 작동원리의 이해(2021),조정효,과학의지평,고등과학원 블로그:https://horizon.kias.re.kr/18474/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d0075f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
