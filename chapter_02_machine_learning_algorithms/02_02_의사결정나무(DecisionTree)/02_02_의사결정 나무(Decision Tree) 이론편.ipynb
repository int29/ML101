{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0722091",
   "metadata": {},
   "source": [
    "## 03.의사결정 나무(Decision Tree) 이론편"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07db51eb",
   "metadata": {},
   "source": [
    "### (1) 의사결정나무 이해하기\n",
    "\n",
    "의사결정 나무(Decision Tree) 알고리즘은 주어진 학습데이터를 트리구조의 모형으로 만들어 데이터를 분류하거나 수치를 예측하는 알고리즘을 말한다. <span style=\"font-size:70%\">[1]</span> \n",
    "\n",
    "대부분의 머신러닝 알고리즘이 결과를 도출하기 까지의 과정 및 해석이 불가능한 블랙박스 모형(Blackbox model)인데 반해,  의사결정트리(Decision Tree)는 결과를 도출하는 과정을 직관적으로 확인하고 해석할 수 있어 굉장히 범용적으로 사용가능하다. 동시에 준수한 성능을 보이기 때문에 굉장히 많이 사용하는 머신러닝 알고리즘이다. <span style=\"font-size:70%\">[2]</span> \n",
    "\n",
    "알고리즘에 대해 깊이 들어가기 전 트리(Tree)가 무엇인지 그리고 학습에 필요한 용어들에 대해서 간단하게 살펴보고 넘어가자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1fa1f9",
   "metadata": {},
   "source": [
    "### (1-1). 트리구조의 이해\n",
    "\n",
    "트리구조는 계층적(hierarchy)인 구조를 갖는 정보를 표현하는 하나의 자료구조이다. 크게 그래프구조라는 자료구조의 한 종류이기 때문에 노드(Node)와 엣지(Edge)로 자료를 계층적으로 표현한다. \n",
    "\n",
    "쉽게 생각해 노드는 데이터를 담는 그릇이고, 각 노드 끼리의 어떻게 연결되었는지에 대한 관계를 엣지로 표현할 수 있다. 이 모습이 나무를 거꾸로 메달아 놓은것과 비슷하다고 하여 트리구조라 부른다.\n",
    "\n",
    "<div align=\"center\">\n",
    "\t<img src=\"https://github.com/int29/PMLP-101/blob/main/chapter_02_machine_learning_algorithms/02_02_%EC%9D%98%EC%82%AC%EA%B2%B0%EC%A0%95%EB%82%98%EB%AC%B4(DecisionTree)/img/03_01.png?raw=true\" width=600>\n",
    "    <p>[그림03_01]</p>\n",
    "</div>\n",
    "\n",
    "의사결정 나무는 트리구조 중 이진트리(binary tree)를 활용한다. 이진트리는 하나의 부모노드(parents node)가 반드시 2개 이하의 자식노드(children node)를 갖는 트리를 말한다. 모든 데이터가 존재하는 가장 첫번째 노드를 뿌리노드(root node)라고 부르며 뿌리노드 부터 자식노드로 계속 데이터가 분할된다. 이 때 더이상 자식노드가 없는 가장 마지막 노드를 잎노드(leaf node)라 부른다. 또한 각 노드의 가로 층을 레벨(Level)이라 부르며 가장 높은 레벨을 트리의 깊이(Depth)라고 부른다.<span style=\"font-size:70%\">[3][4]</span> \n",
    "\n",
    "<div align=\"center\">\n",
    "\t<img src=\"https://github.com/int29/PMLP-101/blob/main/chapter_02_machine_learning_algorithms/02_02_%EC%9D%98%EC%82%AC%EA%B2%B0%EC%A0%95%EB%82%98%EB%AC%B4(DecisionTree)/img/03_02.png?raw=true\" width=700>\n",
    "    <p>[그림03_02]</p>\n",
    "</div>\n",
    "\n",
    "> 구체적인 그래프 구조 및 트리구조에 대하여 확인이 필요하면, 그래프구조 > 트리구조 로 구글링 혹은 아래 2개의 무료 공개수업에서 학습할 수 있다.<br>* 자료구조(2018),조행래,영남대학교,K-Mooc (http://www.kmooc.kr/courses/course-v1:YeungnamUnivK+YU216002+2018_01/about)<br>* 이산수학(2016),원성현,부산가톨릭대학교,KOCW http://www.kocw.net/home/cview.do?cid=2df3e8cf09399ca3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f08091",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "690f38d6",
   "metadata": {},
   "source": [
    "### (1-2). 의사결정나무 학습알고리즘 이해 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd9841d",
   "metadata": {},
   "source": [
    "우리가 스무고개를 통해서 퀴즈를 맞추는 상황을 상상해보자. 스무고개는 단지 맞추는것이 목표인게 아니라 최대한 적은 질문으로 맞추는것이 중요하다. 따라서 정답을 빨리 맞출 수 있도록 `좋은 질문을 하는것`이 중요한다.\n",
    "<br><br>\n",
    "<div align=\"center\">\n",
    "\t<img src=\"https://github.com/int29/PMLP-101/blob/main/chapter_02_machine_learning_algorithms/02_02_%EC%9D%98%EC%82%AC%EA%B2%B0%EC%A0%95%EB%82%98%EB%AC%B4(DecisionTree)/img/03_07.png?raw=true\" width=600>\n",
    "    <p>[그림03_03]</p>\n",
    "</div>\n",
    "\n",
    "왼쪽 처럼 이미 `어떤 동물일까요?`라는 질문에 지구에 산다거나, 동물이냐고 물어보는 질문은 유요한 질문이 아닌 질문수만 차지하는 좋지못한 질문이다. 따라서 오른쪽처럼 질문하기전에 비해 질문후에 정답의 범위가 확 줄어들 수 있는 질문이 필요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6062489",
   "metadata": {},
   "source": [
    "위 스무고개를 의사결정 모델로 구현하면 아래와 같다. 스무고개에서는 처음에는 굉장히 나이브한 질문을 진행하고, 각 질문은 범위를 좁혀나가는 역할을 진행한다. 이렇게 정보를 구체화하다 정답을 도출하게 된다. 각 질문과 질문에 따라 분류되는 추정들은 노드에 저장되고, 내려갈수록 점점 정보가 구체화되고 종국에는 강아지라는 정답을 찾는 전체 과정이 트리모델로 표현된다.<span style=\"font-size:70%\">[5]</span> \n",
    "\n",
    "<div align=\"center\">\n",
    "\t<img src=\"https://github.com/int29/PMLP-101/blob/main/chapter_02_machine_learning_algorithms/02_02_%EC%9D%98%EC%82%AC%EA%B2%B0%EC%A0%95%EB%82%98%EB%AC%B4(DecisionTree)/img/03_03.png?raw=true\" width=400>\n",
    "    <p>[그림03_03]</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa25343",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "의사결정 나무 알고리즘 또한 스무고개와 동일하다. 알고리즘의 목표는 훈련데이터를 트리구조로 표현하는 모델을 만들고 이 모델을 통해 예측값을 만드는것이다. 스무고개에서 정답을 최대한 빨리 찾을 수 있도록 하는 규칙을 `구체화`라는 기준을 활용해 정답을 반복해 찾던 것처럼, 의사결정나무 알고리즘도 각 노드를 정보에서 정답을 최대한 빨리 찾을 수 있는 기준(Criteria)을 통해 데이터를 반복해 나눠가며 정답을 찾는다.\n",
    "\n",
    "의사결정나무 알고리즘에서는 1.지니불순도(Gini impurity), 2.엔트로피(Entropy), 3.분류오차(Classification error) 3가지를 분할기준(Criteria)을 주로 활용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff0ff13",
   "metadata": {},
   "source": [
    "### (1-3) 지니불순도(Gini impurity)를 통한 분할규칙\n",
    "\n",
    "\n",
    "지니불순도(Gini impurity)는 쉽게 어떤 그룹이 한가지로 구성되어 있을 수록(동질성;homogeneity이 높을수록) 선택하기 쉽다는 컨셉을 갖고있다. 이 컨셉은 꾀나 직관적인데 아래그림에서 양끝단으로 갈수록 판단하기에 수월하고 중앙으로 갈수록 점점 판단하기가 애매해지는 것을 확인할 수 있다. 이처럼 지니불순도는 불순도가 낮을수록 정보량이 높아지는 것을 표현한 것을 의미한다.<br><br>\n",
    "\n",
    "<div align=\"center\">\n",
    "\t<img src=\"https://github.com/int29/PMLP-101/blob/main/chapter_02_machine_learning_algorithms/02_02_%EC%9D%98%EC%82%AC%EA%B2%B0%EC%A0%95%EB%82%98%EB%AC%B4(DecisionTree)/img/03_04.png?raw=true\" width=500>\n",
    "    <p>[그림03_04]</p>\n",
    "</div>\n",
    "\n",
    "이처럼 순수하게 데이터내 동질성이 의미하며, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28d6d92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fac4aef8",
   "metadata": {},
   "source": [
    "이제 구체적으로 지니불순도를 어떻게 계산하는지 수학적으로 살펴보자. 지니불순도(Gini Impurity)를 $I_{G}$로 변의상 표현하기로한다. 지니불순도(Gini Impurity)는 아래와 같이 표현할 수 있다. <br><br>\n",
    "\n",
    "$$I_{G} = 1 - Gini$$<br><br>\n",
    "여기서  $Gini$는 각 클래스별 확률을 제곱합한것을 의미한다. 데이터 내 특정 클래스 $i$의 발생확률을 $p_{i}$ 라고 하고 $ p_{i} = \\frac{k_{i}}{N}$ 라면, $Gini$는 아래와 같이 표현할 수 있다.<br><br><br>\n",
    "$$ Gini =(p_{1}^{2}+p_{2}^{2}+p_{3}^{2}+...+p_{j}^{2}) $$<br>\n",
    "$$ \\quad = Gini(D) = \\sum_{i=1}^{j}(\\frac{k_{i}}{N})^2$$\n",
    "\n",
    "따라서 지니불순도(Gini Impurity)는 아래와 같이 표현이 가능하다.\n",
    "\n",
    "$$I_{G} = 1 - (p_{1}^{2}+p_{2}^{2}+p_{3}^{2}+...+p_{n}^{2})= 1 -\\sum_{i=1}^{j}p_{j}^{2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb25cb8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "898d81bf",
   "metadata": {},
   "source": [
    "즉 위 주머니 그림의 지니불순도를 계산하면 아래와 같이 계산이 가능하다.\n",
    "\n",
    "<div align=\"center\">\n",
    "\t<img src=\"https://github.com/int29/PMLP-101/blob/main/chapter_02_machine_learning_algorithms/02_02_%EC%9D%98%EC%82%AC%EA%B2%B0%EC%A0%95%EB%82%98%EB%AC%B4(DecisionTree)/img/03_05.png?raw=true\" width=600>\n",
    "    <p>[그림03_05]</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1e07e6",
   "metadata": {},
   "source": [
    "따라서 의사결정나무 알고리즘은 훈련데이터를 분할할 때, 이 불순도가 가장 낮아질 수 있는 규칙을 통해서 트리를 분할하며, 별도 제약이 없을 경우 모든 잎노드(leaf node)의 지니불순도 값이 0이될 때 까지 트리를 뻗어나간다. 아래 그림에서 주황색 노드는 gini=0이 되는 값으로 나눌 수 있고 gini값이 0이기 때문에 더이상 나누지 않는 것을 알 수 있다.\n",
    "<div align=\"center\">\n",
    "\t<img src=\"https://github.com/int29/PMLP-101/blob/main/chapter_02_machine_learning_algorithms/02_02_%EC%9D%98%EC%82%AC%EA%B2%B0%EC%A0%95%EB%82%98%EB%AC%B4(DecisionTree)/img/03_06.png?raw=true\" width=600>\n",
    "    <p>[그림03_06]</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1f416e",
   "metadata": {},
   "source": [
    "### (1-4) 정보이득(Information Gain)\n",
    "\n",
    "### 엔트로피(Entropy)\n",
    "\n",
    "엔트로피 또한 지니불순도와 컨셉자체는 유사하다. 하나의 정보를 전달하는데 필요한 평균 bit수를 엔트로피(Entropy)라고 한다. 개념은 어렵지만, 쉽게 생각해서 확실하거나 명확하면 어떤것을 설명해야할 평균정보량이 낮고, 복잡하거나 불명확할수록 전달해야할 평균정보량이 많아진다는 것이다. 스무고개를 다시 예로 각 스무고개 단계에서 후보군을 상대방에게 전달해야한다고 해보자. \n",
    "\n",
    "첫번째 질문전에는 생물,무생물 등 어떤 대상일지 모르기 때문에 엄청난 수의 후보군이 존재할것이다.첫번째 질문이 끝나고는 생물 중 총 9개의 동물을 후보군으로 전달했다고 가정해보자. 즉 첫번째 질문전은 굉장히 정보를 많이 전달해야하는 상황이기 때문에 엔트로피가 엄청 높을것이다. 반면에 첫번째 질문이 끝나면 9개의 동물을 전달하면 되기 때문에 엔트로피가 9로 감소한다.\n",
    "\n",
    "두번째 질문후에는 총 6의 엔트로피만 필요하며, 세번째는 3, 네번째는 1로 줄어든다. \n",
    "\n",
    "### 정보이득(Information Gain)과 엔트로피(Entropy)\n",
    "\n",
    "즉 첫번째 질문을 통해서 우리는 무한대의 엔트로피에서 9의 엔트로피로 줄일수 있었고, 줄인 만큼을 정보이득이라 부른다. 두번째 질문은 9엔트로피에서 6엔트로피로 줄이게 되었음으로, 총 3의 정보이득을 얻게된다. 마찬가지로 세번째 질문도 6의 엔트로피를 3으로 줄였기 때문에 3의 정보이득을 갖는다.\n",
    "\n",
    "만약 세번째 질문을 두번째 질문과 바꿔본다면 정보이득의 변화가 있을까? 세번째 질문을 두번째로 변경할 경우 엔트로피는 9에서 5로 감소한다. 따라서 4의 정보이득이 존재하기 때문에 세번째 질문이 두번째 질문으로 오는것이 더 큰 정보이득을 불러온다.\n",
    "\n",
    "따라서 의사결정나무는 이 문제를 해결한다면 더 큰 정보이득을 불러올 수 있는 세번째 질문을 두번째로 변경할 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1570f0d5",
   "metadata": {},
   "source": [
    "마찬가지로 엔트로피와 정보이득을 수학적으로 살펴보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54db2aef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228280cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d770680",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3296878",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32fe41a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1851463",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf51c3f5",
   "metadata": {},
   "source": [
    "#### 인용출처\n",
    "[1] R을 활용한 기계학습, 브레드란츠, 전철욱, 에이콘, 164p ~169p를 참고, 데이터마이닝 기법 비교 연구: 단일 및 복수 의사결정나무, 신은주, 장남식, 363p를 참고<br>\n",
    "[2] R을 활용한 기계학습, 브레드란츠, 전철욱, 에이콘, 164p ~165p를 참고, 파이썬 머신러닝 완벽 가이드, 권민철, 위키북스, 183p 참고<br>\n",
    "[3] 자료구조(2018),조행래,영남대학교,K-Mooc, 8강 트리와 이진 트리의 개념 강의자료 2-6page를 인용하여 재구성함.<br>(http://contents.kocw.or.kr/KOCW/document/2016/cup/weonsumghyun/8.pdf)<br>\n",
    "[4] 이산수학(2016),원성현,부산가톨릭대학교,KOCW  8강 트리 강의자료 3-4page를 인용하여 재구성함.(http://contents.kocw.or.kr/KOCW/document/2016/cup/weonsumghyun/8.pdf)\n",
    "[5] 파이썬을 활용한 머신러닝: 사이킷런 핵심 개발자가 쓴 머신러닝과 데이터 과학 실무서(2018),박해선,김태현,한빛미디어, 101 ~102page 참고하여 재구성함.\n",
    "\n",
    "#### 참고문헌\n",
    "1. R을 활용한 기계학습, 브레드란츠, 전철욱, 에이콘\n",
    "2. 자료구조(2018),조행래,영남대학교,K-Mooc (http://www.kmooc.kr/courses/course-v1:YeungnamUnivK+YU216002+2018_01/about)<br>\n",
    "3. 이산수학(2016),원성현,부산가톨릭대학교,KOCW http://www.kocw.net/home/cview.do?cid=2df3e8cf09399ca3\n",
    "4. 파이썬을 활용한 머신러닝: 사이킷런 핵심 개발자가 쓴 머신러닝과 데이터 과학 실무서, \n",
    "5. 머신러닝에서 딥러닝까지\n",
    "\n",
    "#### 이미지 출처\n",
    "\n",
    "[그림03_01],[그림03_02] : 의사결정나무 차트(https://scikit-learn.org/stable/modules/tree.html#tree)\n",
    "\n",
    "#### 참고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e848a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
